{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Shanaka DeSoysa's Notes \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs gh-deploy - Deploy to GitHub pages. mkdocs -h - Print help message and exit. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to Shanaka DeSoysa's Notes"},{"location":"#welcome-to-shanaka-desoysas-notes","text":"For full documentation visit mkdocs.org .","title":"Welcome to Shanaka DeSoysa's Notes"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs gh-deploy - Deploy to GitHub pages. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Blockchain Explained in 7 Simple Functions \u00b6 Practical hands-on guide to implement your own blockchain with 7 simple Python functions. Hashing Function \u00b6 At the heart of the blockchain is the hashing function. Without encryption, the blockchain will be easily manipulable and transactions will be able to be fraudulently inserted. Here we're using a simple MD5 hashing algorithm. If you're interested in what's actually being used in bitcoin, read here . import hashlib import json def hash_function(k): \"\"\"Hashes our transaction.\"\"\" if type(k) is not str: k = json.dumps(k, sort_keys=True) return hashlib.sha256(k.encode('utf-8')).hexdigest() hash_function('www.geni.ai') '8bfae4a2d420bce8036bd69ed798765a00e103901b7389386373315d7506143f' State Management \u00b6 The \u2018state\u2019 is the record of who owns what. For example, Geni AI have 100 coins and give 5 to John Smith , then the state will be the value of the dictionary below. {'transaction': {'Geni AI': 95, 'John Smith': 5}} def update_state(transaction, state): state = state.copy() for key in transaction: if key in state.keys(): state[key] += transaction[key] else: state[key] = transaction[key] return state Transaction Validation \u00b6 The important thing to note is that overdrafts cannot exist. If there are only 10 coins in existence, then I cannot give 11 coins to someone. The below function verifies that the transaction we attempt to make is indeed valid. Also, a transaction must balance. I cannot give 5 coins and have the recipient receive 4 coins, since that would allow the destruction and creation of coins. def valid_transaction(transaction, state): \"\"\"A valid transaction must sum to 0.\"\"\" if sum(transaction.values()) is not 0: return False for key in transaction.keys(): if key in state.keys(): account_balance = state[key] else: account_balance = 0 if account_balance + transaction[key] < 0: return False return True Make Block \u00b6 Now, we can make our block. The information from the previous block is read, and used to link it to the new block. This, too, is central to the idea of blockchain. Seemingly valid transactions can be attempted to fraudulently be inserted into the blockchain, but decrypting all the previous blocks is computationally (nearly) impossible, which preserves the integrity of the blockchain. def make_block(transactions, chain): \"\"\"Make a block to go into the chain.\"\"\" parent_hash = chain[-1]['hash'] block_number = chain[-1]['contents']['block_number'] + 1 block_contents = { 'block_number': block_number, 'parent_hash': parent_hash, 'transaction_count': block_number + 1, 'transaction': transactions } return {'hash': hash_function(block_contents), 'contents': block_contents} Check Block Hash \u00b6 Below is a small helper function to check the hash of the previous block: def check_block_hash(block): expected_hash = hash_function(block['contents']) if block['hash'] is not expected_hash: raise return Block Validity \u00b6 Once we have assembled everything together, its time to create our block. We will now update the blockchain. def check_block_validity(block, parent, state): parent_number = parent['contents']['block_number'] parent_hash = parent['hash'] block_number = block['contents']['block_number'] for transaction in block['contents']['transaction']: if valid_transaction(transaction, state): state = update_state(transaction, state) else: raise check_block_hash(block) # Check hash integrity if block_number is not parent_number + 1: raise if block['contents']['parent_hash'] is not parent_hash: raise Check Blockchain \u00b6 Before we are finished, the chain must be verified: def check_chain(chain): \"\"\"Check the chain is valid.\"\"\" if type(chain) is str: try: chain = json.loads(chain) assert (type(chain) == list) except ValueError: # String passed in was not valid JSON return False elif type(chain) is not list: return False state = {} for transaction in chain[0]['contents']['transaction']: state = update_state(transaction, state) check_block_hash(chain[0]) parent = chain[0] for block in chain[1:]: state = check_block_validity(block, parent, state) parent = block return state Add transaction \u00b6 Finally, need a transaction function, which hangs all of the above together: def add_transaction_to_chain(transaction, state, chain): if valid_transaction(transaction, state): state = update_state(transaction, state) else: raise Exception('Invalid transaction.') my_block = make_block(state, chain) chain.append(my_block) for transaction in chain: check_chain(transaction) return state, chain Example \u00b6 So, now we have our 7 functions. How do we interact with it? Well, first we need to start our chain with a Genesis Block. This is the inception of our new coin (or stock inventory, etc). For the purposes of this article, I will say that I, Tom, will start off with 10 coins. Let's say we start off with 100 coins for Geni AI . genesis_block = { 'hash': hash_function({ 'block_number': 0, 'parent_hash': None, 'transaction_count': 1, 'transaction': [{'Geni AI': 100}] }), 'contents': { 'block_number': 0, 'parent_hash': None, 'transaction_count': 1, 'transaction': [{'Geni AI': 100}] }, } block_chain = [genesis_block] chain_state = {'Geni AI': 100} Now, look what happens when Geni AI give some coins to user John Smith : chain_state, block_chain = add_transaction_to_chain(transaction={'Geni AI': -5, 'John Smith': 5}, state=chain_state, chain=block_chain) chain_state {'Geni AI': 95, 'John Smith': 5} block_chain [{'contents': {'block_number': 0, 'parent_hash': None, 'transaction': [{'Geni AI': 100}], 'transaction_count': 1}, 'hash': 'e46fb93e96b70a86e6998cd4ba9f20cdbde1e843e5b2343667f0f46d23cee439'}, {'contents': {'block_number': 1, 'parent_hash': 'e46fb93e96b70a86e6998cd4ba9f20cdbde1e843e5b2343667f0f46d23cee439', 'transaction': {'Geni AI': 95, 'John Smith': 5}, 'transaction_count': 2}, 'hash': '65af8f82cd4e55e4db67280eb1cd8c03216ea829ca533135f684524018961c6b'}] Our first new transaction has been created and inserted to the top of the stack. References \u00b6 https://towardsdatascience.com/blockchain-explained-in-7-python-functions-c49c84f34ba5","title":"Blockchain Explained in 7 Simple Functions"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#blockchain-explained-in-7-simple-functions","text":"Practical hands-on guide to implement your own blockchain with 7 simple Python functions.","title":"Blockchain Explained in 7 Simple Functions"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#hashing-function","text":"At the heart of the blockchain is the hashing function. Without encryption, the blockchain will be easily manipulable and transactions will be able to be fraudulently inserted. Here we're using a simple MD5 hashing algorithm. If you're interested in what's actually being used in bitcoin, read here . import hashlib import json def hash_function(k): \"\"\"Hashes our transaction.\"\"\" if type(k) is not str: k = json.dumps(k, sort_keys=True) return hashlib.sha256(k.encode('utf-8')).hexdigest() hash_function('www.geni.ai') '8bfae4a2d420bce8036bd69ed798765a00e103901b7389386373315d7506143f'","title":"Hashing Function"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#state-management","text":"The \u2018state\u2019 is the record of who owns what. For example, Geni AI have 100 coins and give 5 to John Smith , then the state will be the value of the dictionary below. {'transaction': {'Geni AI': 95, 'John Smith': 5}} def update_state(transaction, state): state = state.copy() for key in transaction: if key in state.keys(): state[key] += transaction[key] else: state[key] = transaction[key] return state","title":"State Management"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#transaction-validation","text":"The important thing to note is that overdrafts cannot exist. If there are only 10 coins in existence, then I cannot give 11 coins to someone. The below function verifies that the transaction we attempt to make is indeed valid. Also, a transaction must balance. I cannot give 5 coins and have the recipient receive 4 coins, since that would allow the destruction and creation of coins. def valid_transaction(transaction, state): \"\"\"A valid transaction must sum to 0.\"\"\" if sum(transaction.values()) is not 0: return False for key in transaction.keys(): if key in state.keys(): account_balance = state[key] else: account_balance = 0 if account_balance + transaction[key] < 0: return False return True","title":"Transaction Validation"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#make-block","text":"Now, we can make our block. The information from the previous block is read, and used to link it to the new block. This, too, is central to the idea of blockchain. Seemingly valid transactions can be attempted to fraudulently be inserted into the blockchain, but decrypting all the previous blocks is computationally (nearly) impossible, which preserves the integrity of the blockchain. def make_block(transactions, chain): \"\"\"Make a block to go into the chain.\"\"\" parent_hash = chain[-1]['hash'] block_number = chain[-1]['contents']['block_number'] + 1 block_contents = { 'block_number': block_number, 'parent_hash': parent_hash, 'transaction_count': block_number + 1, 'transaction': transactions } return {'hash': hash_function(block_contents), 'contents': block_contents}","title":"Make Block"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#check-block-hash","text":"Below is a small helper function to check the hash of the previous block: def check_block_hash(block): expected_hash = hash_function(block['contents']) if block['hash'] is not expected_hash: raise return","title":"Check Block Hash"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#block-validity","text":"Once we have assembled everything together, its time to create our block. We will now update the blockchain. def check_block_validity(block, parent, state): parent_number = parent['contents']['block_number'] parent_hash = parent['hash'] block_number = block['contents']['block_number'] for transaction in block['contents']['transaction']: if valid_transaction(transaction, state): state = update_state(transaction, state) else: raise check_block_hash(block) # Check hash integrity if block_number is not parent_number + 1: raise if block['contents']['parent_hash'] is not parent_hash: raise","title":"Block Validity"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#check-blockchain","text":"Before we are finished, the chain must be verified: def check_chain(chain): \"\"\"Check the chain is valid.\"\"\" if type(chain) is str: try: chain = json.loads(chain) assert (type(chain) == list) except ValueError: # String passed in was not valid JSON return False elif type(chain) is not list: return False state = {} for transaction in chain[0]['contents']['transaction']: state = update_state(transaction, state) check_block_hash(chain[0]) parent = chain[0] for block in chain[1:]: state = check_block_validity(block, parent, state) parent = block return state","title":"Check Blockchain"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#add-transaction","text":"Finally, need a transaction function, which hangs all of the above together: def add_transaction_to_chain(transaction, state, chain): if valid_transaction(transaction, state): state = update_state(transaction, state) else: raise Exception('Invalid transaction.') my_block = make_block(state, chain) chain.append(my_block) for transaction in chain: check_chain(transaction) return state, chain","title":"Add transaction"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#example","text":"So, now we have our 7 functions. How do we interact with it? Well, first we need to start our chain with a Genesis Block. This is the inception of our new coin (or stock inventory, etc). For the purposes of this article, I will say that I, Tom, will start off with 10 coins. Let's say we start off with 100 coins for Geni AI . genesis_block = { 'hash': hash_function({ 'block_number': 0, 'parent_hash': None, 'transaction_count': 1, 'transaction': [{'Geni AI': 100}] }), 'contents': { 'block_number': 0, 'parent_hash': None, 'transaction_count': 1, 'transaction': [{'Geni AI': 100}] }, } block_chain = [genesis_block] chain_state = {'Geni AI': 100} Now, look what happens when Geni AI give some coins to user John Smith : chain_state, block_chain = add_transaction_to_chain(transaction={'Geni AI': -5, 'John Smith': 5}, state=chain_state, chain=block_chain) chain_state {'Geni AI': 95, 'John Smith': 5} block_chain [{'contents': {'block_number': 0, 'parent_hash': None, 'transaction': [{'Geni AI': 100}], 'transaction_count': 1}, 'hash': 'e46fb93e96b70a86e6998cd4ba9f20cdbde1e843e5b2343667f0f46d23cee439'}, {'contents': {'block_number': 1, 'parent_hash': 'e46fb93e96b70a86e6998cd4ba9f20cdbde1e843e5b2343667f0f46d23cee439', 'transaction': {'Geni AI': 95, 'John Smith': 5}, 'transaction_count': 2}, 'hash': '65af8f82cd4e55e4db67280eb1cd8c03216ea829ca533135f684524018961c6b'}] Our first new transaction has been created and inserted to the top of the stack.","title":"Example"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#references","text":"https://towardsdatascience.com/blockchain-explained-in-7-python-functions-c49c84f34ba5","title":"References"},{"location":"Docker/Makefile_for_docker-compose/","text":"Makefile for docker-compose \u00b6 Makefile for basic docker-compose commands. ## Project Name ## ## Parameters ##-------------------------|----------------------------------------------------------------- ## COMPOSE_FILE | Name of compose file (docker-compose.yml) ## BUILD_CACHE | Docker build cache (or --no-cache) COMPOSE_FILE : = docker-compose.yml BUILD_CACHE : = #\"--no-cache\" THIS_FILE : = $( lastword $( MAKEFILE_LIST )) CMD_ARGUMENTS ? = $( cmd ) .PHONY: help build up start down destroy stop restart logs logs-web ps ## Commands ##-------------------------|----------------------------------------------------------------- ## help | print docs (make help) or dry run a command (make help cmd=run) help : Makefile ifeq ( $( CMD_ARGUMENTS ) , ) @sed -n 's/^##//p' $< | less else ${ MAKE } $( CMD_ARGUMENTS ) --dry-run endif ## build | docker-compose build build: docker-compose -f $( COMPOSE_FILE ) build $( c ) ## up | docker-compose up up: docker-compose -f $( COMPOSE_FILE ) up -d $( c ) ## start | docker-compose start start: docker-compose -f $( COMPOSE_FILE ) start $( c ) ## down | docker-compose down down: docker-compose -f $( COMPOSE_FILE ) down $( c ) ## destroy | docker-compose down and remove volumes destroy: docker-compose -f $( COMPOSE_FILE ) down -v $( c ) ## stop | docker-compose stop stop: docker-compose -f $( COMPOSE_FILE ) stop $( c ) ## restart | docker-compose stop and up restart: docker-compose -f $( COMPOSE_FILE ) stop $( c ) docker-compose -f $( COMPOSE_FILE ) up -d $( c ) ## logs | docker-compose logs logs: docker-compose -f $( COMPOSE_FILE ) logs --tail = 100 -f $( c ) ## ps | docker-compose ps ps: docker-compose -f $( COMPOSE_FILE ) ps","title":"Makefile for docker-compose"},{"location":"Docker/Makefile_for_docker-compose/#makefile-for-docker-compose","text":"Makefile for basic docker-compose commands. ## Project Name ## ## Parameters ##-------------------------|----------------------------------------------------------------- ## COMPOSE_FILE | Name of compose file (docker-compose.yml) ## BUILD_CACHE | Docker build cache (or --no-cache) COMPOSE_FILE : = docker-compose.yml BUILD_CACHE : = #\"--no-cache\" THIS_FILE : = $( lastword $( MAKEFILE_LIST )) CMD_ARGUMENTS ? = $( cmd ) .PHONY: help build up start down destroy stop restart logs logs-web ps ## Commands ##-------------------------|----------------------------------------------------------------- ## help | print docs (make help) or dry run a command (make help cmd=run) help : Makefile ifeq ( $( CMD_ARGUMENTS ) , ) @sed -n 's/^##//p' $< | less else ${ MAKE } $( CMD_ARGUMENTS ) --dry-run endif ## build | docker-compose build build: docker-compose -f $( COMPOSE_FILE ) build $( c ) ## up | docker-compose up up: docker-compose -f $( COMPOSE_FILE ) up -d $( c ) ## start | docker-compose start start: docker-compose -f $( COMPOSE_FILE ) start $( c ) ## down | docker-compose down down: docker-compose -f $( COMPOSE_FILE ) down $( c ) ## destroy | docker-compose down and remove volumes destroy: docker-compose -f $( COMPOSE_FILE ) down -v $( c ) ## stop | docker-compose stop stop: docker-compose -f $( COMPOSE_FILE ) stop $( c ) ## restart | docker-compose stop and up restart: docker-compose -f $( COMPOSE_FILE ) stop $( c ) docker-compose -f $( COMPOSE_FILE ) up -d $( c ) ## logs | docker-compose logs logs: docker-compose -f $( COMPOSE_FILE ) logs --tail = 100 -f $( c ) ## ps | docker-compose ps ps: docker-compose -f $( COMPOSE_FILE ) ps","title":"Makefile for docker-compose"},{"location":"Linux/Change_Ownership/","text":"Chnage Ownership \u00b6 To change the ownership of all the files in current directory. sudo chown -R $USER : .","title":"Change Ownership of Files"},{"location":"Linux/Change_Ownership/#chnage-ownership","text":"To change the ownership of all the files in current directory. sudo chown -R $USER : .","title":"Chnage Ownership"},{"location":"Linux/Check_Group_Membership/","text":"Check Group Membership \u00b6 id $USER | tr ',' '\\n'","title":"Check Group Membership"},{"location":"Linux/Check_Group_Membership/#check-group-membership","text":"id $USER | tr ',' '\\n'","title":"Check Group Membership"},{"location":"Linux/Dictionary_Words/","text":"Unix Words File \u00b6 Dictionary Words is a standard file on Unix and Unix-like operating systems, and is simply a newline-delimited list of dictionary words. It is used, for instance, by spell-checking programs. cat /usr/share/dict/words","title":"Unix Dictionary Words File"},{"location":"Linux/Dictionary_Words/#unix-words-file","text":"Dictionary Words is a standard file on Unix and Unix-like operating systems, and is simply a newline-delimited list of dictionary words. It is used, for instance, by spell-checking programs. cat /usr/share/dict/words","title":"Unix Words File"},{"location":"Linux/List_files_exceeding_size/","text":"List Files Exceeding Certain Size \u00b6 List Files Exceeding Certain Size in current (or given) directory find . -maxdepth 1 -type f -size +10M","title":"List Files Exceeding Certain Size"},{"location":"Linux/List_files_exceeding_size/#list-files-exceeding-certain-size","text":"List Files Exceeding Certain Size in current (or given) directory find . -maxdepth 1 -type f -size +10M","title":"List Files Exceeding Certain Size"},{"location":"Linux/Random_Password/","text":"Random Password Generator using openssl \u00b6 Password with Hexadecimal Characters \u00b6 Specify number of characters, 32 in this example. openssl rand -hex 32 Example Output 35e29ce60b04a7cafb7790a62805d5ed62ff012cbb51fa2f1bb40cb94a356b9b Password with Base64 Characters \u00b6 Specify number of characters, 16 in this example. openssl rand -base64 16 Example Output buARrzhKpgnDaWI6lAzsRA==","title":"Random Password Generator"},{"location":"Linux/Random_Password/#random-password-generator-using-openssl","text":"","title":"Random Password Generator using openssl"},{"location":"Linux/Random_Password/#password-with-hexadecimal-characters","text":"Specify number of characters, 32 in this example. openssl rand -hex 32 Example Output 35e29ce60b04a7cafb7790a62805d5ed62ff012cbb51fa2f1bb40cb94a356b9b","title":"Password with Hexadecimal Characters"},{"location":"Linux/Random_Password/#password-with-base64-characters","text":"Specify number of characters, 16 in this example. openssl rand -base64 16 Example Output buARrzhKpgnDaWI6lAzsRA==","title":"Password with Base64 Characters"},{"location":"Linux/Symbolic_Link/","text":"Symbolic link \u00b6 Symbolic link In computing, a symbolic link (also symlink or soft link) is a term for any file that contains a reference to another file or directory in the form of an absolute or relative path and that affects pathname resolution. readlink /usr/share/dict/words","title":"Symbolic link"},{"location":"Linux/Symbolic_Link/#symbolic-link","text":"Symbolic link In computing, a symbolic link (also symlink or soft link) is a term for any file that contains a reference to another file or directory in the form of an absolute or relative path and that affects pathname resolution. readlink /usr/share/dict/words","title":"Symbolic link"},{"location":"ML_with_Graphs/CS224W_Colab_0/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); CS224W - Colab 0 \u00b6 Colab 0 will not be graded , so you don't need to hand in this notebook. That said, we highly recommend you to run this notebook, so you can get familiar with the basic concepts of graph mining and Graph Neural Networks. In this Colab, we will introduce two packages, NetworkX and PyTorch Geometric . For the PyTorch Geometric section, you don't need to understand all the details already. Concepts and implementations of graph neural network will be covered in future lectures and Colabs. Please make a copy before you proceed. New Section \u00b6 NetworkX Tutorial \u00b6 NetworkX is one of the most frequently used Python packages to create, manipulate, and mine graphs. Main parts of this tutorial are adapted from https://colab.research.google.com/github/jdwittenauer/ipython-notebooks/blob/master/notebooks/libraries/NetworkX.ipynb#scrollTo=zA1OO6huHeV6 Setup \u00b6 # Import the NetworkX package import networkx as nx Graph \u00b6 NetworkX provides several classes to store different types of graphs, such as directed and undirected graph. It also provides classes to create multigraphs (both directed and undirected). For more information, please refer to NetworkX graph types . # Create an undirected graph G G = nx . Graph () print ( G . is_directed ()) # Create a directed graph H H = nx . DiGraph () print ( H . is_directed ()) # Add graph level attribute G . graph [ \"Name\" ] = \"Bar\" print ( G . graph ) False True {'Name': 'Bar'} Node \u00b6 Nodes (with attributes) can be easily added to NetworkX graphs. # Add one node with node level attributes G . add_node ( 0 , feature = 0 , label = 0 ) # Get attributes of the node 0 node_0_attr = G . nodes [ 0 ] print ( \"Node 0 has the attributes {} \" . format ( node_0_attr )) Node 0 has the attributes {'feature': 0, 'label': 0} # Add multiple nodes with attributes G . add_nodes_from ([ ( 1 , { \"feature\" : 1 , \"label\" : 1 }), ( 2 , { \"feature\" : 2 , \"label\" : 2 }) ]) # Loop through all the nodes # Set data=True will return node attributes for node in G . nodes ( data = True ): print ( node ) # Get number of nodes num_nodes = G . number_of_nodes () print ( \"G has {} nodes\" . format ( num_nodes )) (0, {'feature': 0, 'label': 0}) (1, {'feature': 1, 'label': 1}) (2, {'feature': 2, 'label': 2}) G has 3 nodes Edge \u00b6 Similar to nodes, edges (with attributes) can also be easily added to NetworkX graphs. # Add one edge with edge weight 0.5 G . add_edge ( 0 , 1 , weight = 0.5 ) # Get attributes of the edge (0, 1) edge_0_1_attr = G . edges [( 0 , 1 )] print ( \"Edge (0, 1) has the attributes {} \" . format ( edge_0_1_attr )) Edge (0, 1) has the attributes {'weight': 0.5} # Add multiple edges with edge weights G . add_edges_from ([ ( 1 , 2 , { \"weight\" : 0.3 }), ( 2 , 0 , { \"weight\" : 0.1 }) ]) # Loop through all the edges # Here there is no data=True, so only the edge will be returned for edge in G . edges (): print ( edge ) # Get number of edges num_edges = G . number_of_edges () print ( \"G has {} edges\" . format ( num_edges )) (0, 1) (0, 2) (1, 2) G has 3 edges Visualization \u00b6 # Draw the graph nx . draw ( G , with_labels = True ) Node Degree and Neighbor \u00b6 node_id = 1 # Degree of node 1 print ( \"Node {} has degree {} \" . format ( node_id , G . degree [ node_id ])) # Get neighbor of node 1 for neighbor in G . neighbors ( node_id ): print ( \"Node {} has neighbor {} \" . format ( node_id , neighbor )) Node 1 has degree 2 Node 1 has neighbor 0 Node 1 has neighbor 2 Other Functionalities \u00b6 NetworkX also provides plenty of useful methods to study graphs. Here is an example to get PageRank of nodes (we will talk about PageRank in one of the future lectures). num_nodes = 4 # Create a new path like graph and change it to a directed graph G = nx . DiGraph ( nx . path_graph ( num_nodes )) nx . draw ( G , with_labels = True ) # Get the PageRank pr = nx . pagerank ( G , alpha = 0.8 ) pr {0: 0.17857162031103999, 1: 0.32142837968896, 2: 0.32142837968896, 3: 0.17857162031103999} Documentation \u00b6 You can explore more NetworkX functions through its documentation . PyTorch Geometric Tutorial \u00b6 PyTorch Geometric (PyG) is an extension library for PyTorch. It provides useful primitives to develop Graph Deep Learning models, including various graph neural network layers and a large number of benchmark datasets. Don't worry if you don't understand some concepts such as GCNConv -- we will cover all of them in the future lectures :) This tutorial is adapted from https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8?usp=sharing#scrollTo=ci-LpZWhRJoI by Matthias Fey import torch print ( \"PyTorch has version {} \" . format ( torch . __version__ )) PyTorch has version 1.8.1+cu101 Setup \u00b6 The installation of PyG on Colab can be a little bit tricky. Execute the cell below -- in case of issues, more information can be found on the PyG's installation page . # Install torch geometric ! pip install - q torch - scatter - f https : // pytorch - geometric . com / whl / torch - 1.8 . 0 + cu101 . html ! pip install - q torch - sparse - f https : // pytorch - geometric . com / whl / torch - 1.8 . 0 + cu101 . html ! pip install - q torch - geometric |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.6MB 9.5MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.5MB 9.8MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 10.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235kB 27.4MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.2MB 28.6MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 3.7MB/s Building wheel for torch-geometric (setup.py) ... done Visualization \u00b6 # Helper function for visualization. % matplotlib inline import torch import networkx as nx import matplotlib.pyplot as plt # Visualization function for NX graph or PyTorch tensor def visualize ( h , color , epoch = None , loss = None ): plt . figure ( figsize = ( 7 , 7 )) plt . xticks ([]) plt . yticks ([]) if torch . is_tensor ( h ): h = h . detach () . cpu () . numpy () plt . scatter ( h [:, 0 ], h [:, 1 ], s = 140 , c = color , cmap = \"Set2\" ) if epoch is not None and loss is not None : plt . xlabel ( f 'Epoch: { epoch } , Loss: { loss . item () : .4f } ' , fontsize = 16 ) else : nx . draw_networkx ( G , pos = nx . spring_layout ( G , seed = 42 ), with_labels = False , node_color = color , cmap = \"Set2\" ) plt . show () Introduction \u00b6 Recently, deep learning on graphs has emerged to one of the hottest research fields in the deep learning community. Here, Graph Neural Networks (GNNs) aim to generalize classical deep learning concepts to irregular structured data (in contrast to images or texts) and to enable neural networks to reason about objects and their relations. This tutorial will introduce you to some fundamental concepts regarding deep learning on graphs via Graph Neural Networks based on the PyTorch Geometric (PyG) library . PyTorch Geometric is an extension library to the popular deep learning framework PyTorch , and consists of various methods and utilities to ease the implementation of Graph Neural Networks. Following Kipf et al. (2017) , let's dive into the world of GNNs by looking at a simple graph-structured example, the well-known Zachary's karate club network . This graph describes a social network of 34 members of a karate club and documents links between members who interacted outside the club. Here, we are interested in detecting communities that arise from the member's interaction. Dataset \u00b6 PyTorch Geometric provides an easy access to the dataset via the torch_geometric.datasets subpackage: from torch_geometric.datasets import KarateClub dataset = KarateClub () print ( f 'Dataset: { dataset } :' ) print ( '======================' ) print ( f 'Number of graphs: { len ( dataset ) } ' ) print ( f 'Number of features: { dataset . num_features } ' ) print ( f 'Number of classes: { dataset . num_classes } ' ) Dataset: KarateClub(): ====================== Number of graphs: 1 Number of features: 34 Number of classes: 4 After initializing the KarateClub dataset, we first can inspect some of its properties. For example, we can see that this dataset holds exactly one graph , and that each node in this dataset is assigned a 34-dimensional feature vector (which uniquely describes the members of the karate club). Furthermore, the graph holds exactly 4 classes , which represent the community each node belongs to. Let's now look at the underlying graph in more detail: data . keys ['x', 'edge_index', 'y', 'train_mask'] data = dataset [ 0 ] # Get the first graph object. print ( data ) print ( '==============================================================' ) # Gather some statistics about the graph. print ( f 'Number of nodes: { data . num_nodes } ' ) print ( f 'Number of edges: { data . num_edges } ' ) print ( f 'Average node degree: { data . num_edges / data . num_nodes : .2f } ' ) print ( f 'Number of training nodes: { data . train_mask . sum () } ' ) print ( f 'Training node label rate: { int ( data . train_mask . sum ()) / data . num_nodes : .2f } ' ) print ( f 'Contains isolated nodes: { data . contains_isolated_nodes () } ' ) print ( f 'Contains self-loops: { data . contains_self_loops () } ' ) print ( f 'Is undirected: { data . is_undirected () } ' ) Data(edge_index=[2, 156], train_mask=[34], x=[34, 34], y=[34]) ============================================================== Number of nodes: 34 Number of edges: 156 Average node degree: 4.59 Number of training nodes: 4 Training node label rate: 0.12 Contains isolated nodes: False Contains self-loops: False Is undirected: True Data \u00b6 Each graph in PyTorch Geometric is represented by a single Data object, which holds all the information to describe its graph representation. We can print the data object anytime via print(data) to receive a short summary about its attributes and their shapes: Data(edge_index=[2, 156], x=[34, 34], y=[34], train_mask=[34]) We can see that this data object holds 4 attributes: (1) The edge_index property holds the information about the graph connectivity , i.e. , a tuple of source and destination node indices for each edge. PyG further refers to (2) node features as x (each of the 34 nodes is assigned a 34-dim feature vector), and to (3) node labels as y (each node is assigned to exactly one class). (4) There also exists an additional attribute called train_mask , which describes for which nodes we already know their community assigments. In total, we are only aware of the ground-truth labels of 4 nodes (one for each community), and the task is to infer the community assignment for the remaining nodes. The data object also provides some utility functions to infer some basic properties of the underlying graph. For example, we can easily infer whether there exists isolated nodes in the graph ( i.e. there exists no edge to any node), whether the graph contains self-loops ( i.e. , $(v, v) \\in \\mathcal{E}$), or whether the graph is undirected ( i.e. , for each edge $(v, w) \\in \\mathcal{E}$ there also exists the edge $(w, v) \\in \\mathcal{E}$). from IPython.display import Javascript # Restrict height of output cell. display ( Javascript ( '''google.colab.output.setIframeHeight(0, true, {maxHeight: 300} )''' )) edge_index = data . edge_index print ( edge_index . t ()) var element = $('#5f578120-a8b7-4436-ba7d-0b6fb3047cb7'); google.colab.output.setIframeHeight(0, true, {maxHeight: 300}) tensor([[ 0, 1], [ 0, 2], [ 0, 3], [ 0, 4], [ 0, 5], [ 0, 6], [ 0, 7], [ 0, 8], [ 0, 10], [ 0, 11], [ 0, 12], [ 0, 13], [ 0, 17], [ 0, 19], [ 0, 21], [ 0, 31], [ 1, 0], [ 1, 2], [ 1, 3], [ 1, 7], [ 1, 13], [ 1, 17], [ 1, 19], [ 1, 21], [ 1, 30], [ 2, 0], [ 2, 1], [ 2, 3], [ 2, 7], [ 2, 8], [ 2, 9], [ 2, 13], [ 2, 27], [ 2, 28], [ 2, 32], [ 3, 0], [ 3, 1], [ 3, 2], [ 3, 7], [ 3, 12], [ 3, 13], [ 4, 0], [ 4, 6], [ 4, 10], [ 5, 0], [ 5, 6], [ 5, 10], [ 5, 16], [ 6, 0], [ 6, 4], [ 6, 5], [ 6, 16], [ 7, 0], [ 7, 1], [ 7, 2], [ 7, 3], [ 8, 0], [ 8, 2], [ 8, 30], [ 8, 32], [ 8, 33], [ 9, 2], [ 9, 33], [10, 0], [10, 4], [10, 5], [11, 0], [12, 0], [12, 3], [13, 0], [13, 1], [13, 2], [13, 3], [13, 33], [14, 32], [14, 33], [15, 32], [15, 33], [16, 5], [16, 6], [17, 0], [17, 1], [18, 32], [18, 33], [19, 0], [19, 1], [19, 33], [20, 32], [20, 33], [21, 0], [21, 1], [22, 32], [22, 33], [23, 25], [23, 27], [23, 29], [23, 32], [23, 33], [24, 25], [24, 27], [24, 31], [25, 23], [25, 24], [25, 31], [26, 29], [26, 33], [27, 2], [27, 23], [27, 24], [27, 33], [28, 2], [28, 31], [28, 33], [29, 23], [29, 26], [29, 32], [29, 33], [30, 1], [30, 8], [30, 32], [30, 33], [31, 0], [31, 24], [31, 25], [31, 28], [31, 32], [31, 33], [32, 2], [32, 8], [32, 14], [32, 15], [32, 18], [32, 20], [32, 22], [32, 23], [32, 29], [32, 30], [32, 31], [32, 33], [33, 8], [33, 9], [33, 13], [33, 14], [33, 15], [33, 18], [33, 19], [33, 20], [33, 22], [33, 23], [33, 26], [33, 27], [33, 28], [33, 29], [33, 30], [33, 31], [33, 32]]) Edge Index \u00b6 By printing edge_index , we can further understand how PyG represents graph connectivity internally. We can see that for each edge, edge_index holds a tuple of two node indices, where the first value describes the node index of the source node and the second value describes the node index of the destination node of an edge. This representation is known as the COO format (coordinate format) commonly used for representing sparse matrices. Instead of holding the adjacency information in a dense representation $\\mathbf{A} \\in { 0, 1 }^{|\\mathcal{V}| \\times |\\mathcal{V}|}$, PyG represents graphs sparsely, which refers to only holding the coordinates/values for which entries in $\\mathbf{A}$ are non-zero. We can further visualize the graph by converting it to the networkx library format, which implements, in addition to graph manipulation functionalities, powerful tools for visualization: from torch_geometric.utils import to_networkx G = to_networkx ( data , to_undirected = True ) visualize ( G , color = data . y ) Implementing Graph Neural Networks \u00b6 After learning about PyG's data handling, it's time to implement our first Graph Neural Network! For this, we will use one of the most simple GNN operators, the GCN layer ( Kipf et al. (2017) ). PyG implements this layer via GCNConv , which can be executed by passing in the node feature representation x and the COO graph connectivity representation edge_index . With this, we are ready to create our first Graph Neural Network by defining our network architecture in a torch.nn.Module class: import torch from torch.nn import Linear from torch_geometric.nn import GCNConv class GCN ( torch . nn . Module ): def __init__ ( self ): super ( GCN , self ) . __init__ () torch . manual_seed ( 12345 ) self . conv1 = GCNConv ( dataset . num_features , 4 ) self . conv2 = GCNConv ( 4 , 4 ) self . conv3 = GCNConv ( 4 , 2 ) self . classifier = Linear ( 2 , dataset . num_classes ) def forward ( self , x , edge_index ): h = self . conv1 ( x , edge_index ) h = h . tanh () h = self . conv2 ( h , edge_index ) h = h . tanh () h = self . conv3 ( h , edge_index ) h = h . tanh () # Final GNN embedding space. # Apply a final (linear) classifier. out = self . classifier ( h ) return out , h model = GCN () print ( model ) GCN( (conv1): GCNConv(34, 4) (conv2): GCNConv(4, 4) (conv3): GCNConv(4, 2) (classifier): Linear(in_features=2, out_features=4, bias=True) ) Here, we first initialize all of our building blocks in __init__ and define the computation flow of our network in forward . We first define and stack three graph convolution layers , which corresponds to aggregating 3-hop neighborhood information around each node (all nodes up to 3 \"hops\" away). In addition, the GCNConv layers reduce the node feature dimensionality to $2$, i.e. , $34 \\rightarrow 4 \\rightarrow 4 \\rightarrow 2$. Each GCNConv layer is enhanced by a tanh non-linearity. After that, we apply a single linear transformation ( torch.nn.Linear ) that acts as a classifier to map our nodes to 1 out of the 4 classes/communities. We return both the output of the final classifier as well as the final node embeddings produced by our GNN. We proceed to initialize our final model via GCN() , and printing our model produces a summary of all its used sub-modules. model = GCN () _ , h = model ( data . x , data . edge_index ) print ( f 'Embedding shape: { list ( h . shape ) } ' ) visualize ( h , color = data . y ) Embedding shape: [34, 2] Remarkably, even before training the weights of our model, the model produces an embedding of nodes that closely resembles the community-structure of the graph. Nodes of the same color (community) are already closely clustered together in the embedding space, although the weights of our model are initialized completely at random and we have not yet performed any training so far! This leads to the conclusion that GNNs introduce a strong inductive bias, leading to similar embeddings for nodes that are close to each other in the input graph. Training on the Karate Club Network \u00b6 But can we do better? Let's look at an example on how to train our network parameters based on the knowledge of the community assignments of 4 nodes in the graph (one for each community): Since everything in our model is differentiable and parameterized, we can add some labels, train the model and observe how the embeddings react. Here, we make use of a semi-supervised or transductive learning procedure: We simply train against one node per class, but are allowed to make use of the complete input graph data. Training our model is very similar to any other PyTorch model. In addition to defining our network architecture, we define a loss critertion (here, CrossEntropyLoss ) and initialize a stochastic gradient optimizer (here, Adam ). After that, we perform multiple rounds of optimization, where each round consists of a forward and backward pass to compute the gradients of our model parameters w.r.t. to the loss derived from the forward pass. If you are not new to PyTorch, this scheme should appear familar to you. Otherwise, the PyTorch docs provide a good introduction on how to train a neural network in PyTorch . Note that our semi-supervised learning scenario is achieved by the following line: loss = criterion(out[data.train_mask], data.y[data.train_mask]) While we compute node embeddings for all of our nodes, we only make use of the training nodes for computing the loss . Here, this is implemented by filtering the output of the classifier out and ground-truth labels data.y to only contain the nodes in the train_mask . Let us now start training and see how our node embeddings evolve over time (best experienced by explicitely running the code): import time from IPython.display import Javascript # Restrict height of output cell. display ( Javascript ( '''google.colab.output.setIframeHeight(0, true, {maxHeight: 430} )''' )) model = GCN () criterion = torch . nn . CrossEntropyLoss () # Define loss criterion. optimizer = torch . optim . Adam ( model . parameters (), lr = 0.01 ) # Define optimizer. def train ( data ): optimizer . zero_grad () # Clear gradients. out , h = model ( data . x , data . edge_index ) # Perform a single forward pass. loss = criterion ( out [ data . train_mask ], data . y [ data . train_mask ]) # Compute the loss solely based on the training nodes. loss . backward () # Derive gradients. optimizer . step () # Update parameters based on gradients. return loss , h for epoch in range ( 401 ): loss , h = train ( data ) # Visualize the node embeddings every 10 epochs if epoch % 10 == 0 : visualize ( h , color = data . y , epoch = epoch , loss = loss ) time . sleep ( 0.3 ) var element = $('#7fb53f8a-bbaa-4cb9-b7dc-1fc52234e214'); google.colab.output.setIframeHeight(0, true, {maxHeight: 430}) As one can see, our 3-layer GCN model manages to linearly separating the communities and classifying most of the nodes correctly. Furthermore, we did this all with a few lines of code, thanks to the PyTorch Geometric library which helped us out with data handling and GNN implementations. Documentation \u00b6 You can explore more PyG functions through its documentation .","title":"CS224W Colab 0"},{"location":"ML_with_Graphs/CS224W_Colab_0/#cs224w-colab-0","text":"Colab 0 will not be graded , so you don't need to hand in this notebook. That said, we highly recommend you to run this notebook, so you can get familiar with the basic concepts of graph mining and Graph Neural Networks. In this Colab, we will introduce two packages, NetworkX and PyTorch Geometric . For the PyTorch Geometric section, you don't need to understand all the details already. Concepts and implementations of graph neural network will be covered in future lectures and Colabs. Please make a copy before you proceed.","title":"CS224W - Colab 0"},{"location":"ML_with_Graphs/CS224W_Colab_0/#new-section","text":"","title":"New Section"},{"location":"ML_with_Graphs/CS224W_Colab_0/#networkx-tutorial","text":"NetworkX is one of the most frequently used Python packages to create, manipulate, and mine graphs. Main parts of this tutorial are adapted from https://colab.research.google.com/github/jdwittenauer/ipython-notebooks/blob/master/notebooks/libraries/NetworkX.ipynb#scrollTo=zA1OO6huHeV6","title":"NetworkX Tutorial"},{"location":"ML_with_Graphs/CS224W_Colab_0/#setup","text":"# Import the NetworkX package import networkx as nx","title":"Setup"},{"location":"ML_with_Graphs/CS224W_Colab_0/#graph","text":"NetworkX provides several classes to store different types of graphs, such as directed and undirected graph. It also provides classes to create multigraphs (both directed and undirected). For more information, please refer to NetworkX graph types . # Create an undirected graph G G = nx . Graph () print ( G . is_directed ()) # Create a directed graph H H = nx . DiGraph () print ( H . is_directed ()) # Add graph level attribute G . graph [ \"Name\" ] = \"Bar\" print ( G . graph ) False True {'Name': 'Bar'}","title":"Graph"},{"location":"ML_with_Graphs/CS224W_Colab_0/#node","text":"Nodes (with attributes) can be easily added to NetworkX graphs. # Add one node with node level attributes G . add_node ( 0 , feature = 0 , label = 0 ) # Get attributes of the node 0 node_0_attr = G . nodes [ 0 ] print ( \"Node 0 has the attributes {} \" . format ( node_0_attr )) Node 0 has the attributes {'feature': 0, 'label': 0} # Add multiple nodes with attributes G . add_nodes_from ([ ( 1 , { \"feature\" : 1 , \"label\" : 1 }), ( 2 , { \"feature\" : 2 , \"label\" : 2 }) ]) # Loop through all the nodes # Set data=True will return node attributes for node in G . nodes ( data = True ): print ( node ) # Get number of nodes num_nodes = G . number_of_nodes () print ( \"G has {} nodes\" . format ( num_nodes )) (0, {'feature': 0, 'label': 0}) (1, {'feature': 1, 'label': 1}) (2, {'feature': 2, 'label': 2}) G has 3 nodes","title":"Node"},{"location":"ML_with_Graphs/CS224W_Colab_0/#edge","text":"Similar to nodes, edges (with attributes) can also be easily added to NetworkX graphs. # Add one edge with edge weight 0.5 G . add_edge ( 0 , 1 , weight = 0.5 ) # Get attributes of the edge (0, 1) edge_0_1_attr = G . edges [( 0 , 1 )] print ( \"Edge (0, 1) has the attributes {} \" . format ( edge_0_1_attr )) Edge (0, 1) has the attributes {'weight': 0.5} # Add multiple edges with edge weights G . add_edges_from ([ ( 1 , 2 , { \"weight\" : 0.3 }), ( 2 , 0 , { \"weight\" : 0.1 }) ]) # Loop through all the edges # Here there is no data=True, so only the edge will be returned for edge in G . edges (): print ( edge ) # Get number of edges num_edges = G . number_of_edges () print ( \"G has {} edges\" . format ( num_edges )) (0, 1) (0, 2) (1, 2) G has 3 edges","title":"Edge"},{"location":"ML_with_Graphs/CS224W_Colab_0/#visualization","text":"# Draw the graph nx . draw ( G , with_labels = True )","title":"Visualization"},{"location":"ML_with_Graphs/CS224W_Colab_0/#node-degree-and-neighbor","text":"node_id = 1 # Degree of node 1 print ( \"Node {} has degree {} \" . format ( node_id , G . degree [ node_id ])) # Get neighbor of node 1 for neighbor in G . neighbors ( node_id ): print ( \"Node {} has neighbor {} \" . format ( node_id , neighbor )) Node 1 has degree 2 Node 1 has neighbor 0 Node 1 has neighbor 2","title":"Node Degree and Neighbor"},{"location":"ML_with_Graphs/CS224W_Colab_0/#other-functionalities","text":"NetworkX also provides plenty of useful methods to study graphs. Here is an example to get PageRank of nodes (we will talk about PageRank in one of the future lectures). num_nodes = 4 # Create a new path like graph and change it to a directed graph G = nx . DiGraph ( nx . path_graph ( num_nodes )) nx . draw ( G , with_labels = True ) # Get the PageRank pr = nx . pagerank ( G , alpha = 0.8 ) pr {0: 0.17857162031103999, 1: 0.32142837968896, 2: 0.32142837968896, 3: 0.17857162031103999}","title":"Other Functionalities"},{"location":"ML_with_Graphs/CS224W_Colab_0/#documentation","text":"You can explore more NetworkX functions through its documentation .","title":"Documentation"},{"location":"ML_with_Graphs/CS224W_Colab_0/#pytorch-geometric-tutorial","text":"PyTorch Geometric (PyG) is an extension library for PyTorch. It provides useful primitives to develop Graph Deep Learning models, including various graph neural network layers and a large number of benchmark datasets. Don't worry if you don't understand some concepts such as GCNConv -- we will cover all of them in the future lectures :) This tutorial is adapted from https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8?usp=sharing#scrollTo=ci-LpZWhRJoI by Matthias Fey import torch print ( \"PyTorch has version {} \" . format ( torch . __version__ )) PyTorch has version 1.8.1+cu101","title":"PyTorch Geometric Tutorial"},{"location":"ML_with_Graphs/CS224W_Colab_0/#setup_1","text":"The installation of PyG on Colab can be a little bit tricky. Execute the cell below -- in case of issues, more information can be found on the PyG's installation page . # Install torch geometric ! pip install - q torch - scatter - f https : // pytorch - geometric . com / whl / torch - 1.8 . 0 + cu101 . html ! pip install - q torch - sparse - f https : // pytorch - geometric . com / whl / torch - 1.8 . 0 + cu101 . html ! pip install - q torch - geometric |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.6MB 9.5MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.5MB 9.8MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 10.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235kB 27.4MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.2MB 28.6MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 3.7MB/s Building wheel for torch-geometric (setup.py) ... done","title":"Setup"},{"location":"ML_with_Graphs/CS224W_Colab_0/#visualization_1","text":"# Helper function for visualization. % matplotlib inline import torch import networkx as nx import matplotlib.pyplot as plt # Visualization function for NX graph or PyTorch tensor def visualize ( h , color , epoch = None , loss = None ): plt . figure ( figsize = ( 7 , 7 )) plt . xticks ([]) plt . yticks ([]) if torch . is_tensor ( h ): h = h . detach () . cpu () . numpy () plt . scatter ( h [:, 0 ], h [:, 1 ], s = 140 , c = color , cmap = \"Set2\" ) if epoch is not None and loss is not None : plt . xlabel ( f 'Epoch: { epoch } , Loss: { loss . item () : .4f } ' , fontsize = 16 ) else : nx . draw_networkx ( G , pos = nx . spring_layout ( G , seed = 42 ), with_labels = False , node_color = color , cmap = \"Set2\" ) plt . show ()","title":"Visualization"},{"location":"ML_with_Graphs/CS224W_Colab_0/#introduction","text":"Recently, deep learning on graphs has emerged to one of the hottest research fields in the deep learning community. Here, Graph Neural Networks (GNNs) aim to generalize classical deep learning concepts to irregular structured data (in contrast to images or texts) and to enable neural networks to reason about objects and their relations. This tutorial will introduce you to some fundamental concepts regarding deep learning on graphs via Graph Neural Networks based on the PyTorch Geometric (PyG) library . PyTorch Geometric is an extension library to the popular deep learning framework PyTorch , and consists of various methods and utilities to ease the implementation of Graph Neural Networks. Following Kipf et al. (2017) , let's dive into the world of GNNs by looking at a simple graph-structured example, the well-known Zachary's karate club network . This graph describes a social network of 34 members of a karate club and documents links between members who interacted outside the club. Here, we are interested in detecting communities that arise from the member's interaction.","title":"Introduction"},{"location":"ML_with_Graphs/CS224W_Colab_0/#dataset","text":"PyTorch Geometric provides an easy access to the dataset via the torch_geometric.datasets subpackage: from torch_geometric.datasets import KarateClub dataset = KarateClub () print ( f 'Dataset: { dataset } :' ) print ( '======================' ) print ( f 'Number of graphs: { len ( dataset ) } ' ) print ( f 'Number of features: { dataset . num_features } ' ) print ( f 'Number of classes: { dataset . num_classes } ' ) Dataset: KarateClub(): ====================== Number of graphs: 1 Number of features: 34 Number of classes: 4 After initializing the KarateClub dataset, we first can inspect some of its properties. For example, we can see that this dataset holds exactly one graph , and that each node in this dataset is assigned a 34-dimensional feature vector (which uniquely describes the members of the karate club). Furthermore, the graph holds exactly 4 classes , which represent the community each node belongs to. Let's now look at the underlying graph in more detail: data . keys ['x', 'edge_index', 'y', 'train_mask'] data = dataset [ 0 ] # Get the first graph object. print ( data ) print ( '==============================================================' ) # Gather some statistics about the graph. print ( f 'Number of nodes: { data . num_nodes } ' ) print ( f 'Number of edges: { data . num_edges } ' ) print ( f 'Average node degree: { data . num_edges / data . num_nodes : .2f } ' ) print ( f 'Number of training nodes: { data . train_mask . sum () } ' ) print ( f 'Training node label rate: { int ( data . train_mask . sum ()) / data . num_nodes : .2f } ' ) print ( f 'Contains isolated nodes: { data . contains_isolated_nodes () } ' ) print ( f 'Contains self-loops: { data . contains_self_loops () } ' ) print ( f 'Is undirected: { data . is_undirected () } ' ) Data(edge_index=[2, 156], train_mask=[34], x=[34, 34], y=[34]) ============================================================== Number of nodes: 34 Number of edges: 156 Average node degree: 4.59 Number of training nodes: 4 Training node label rate: 0.12 Contains isolated nodes: False Contains self-loops: False Is undirected: True","title":"Dataset"},{"location":"ML_with_Graphs/CS224W_Colab_0/#data","text":"Each graph in PyTorch Geometric is represented by a single Data object, which holds all the information to describe its graph representation. We can print the data object anytime via print(data) to receive a short summary about its attributes and their shapes: Data(edge_index=[2, 156], x=[34, 34], y=[34], train_mask=[34]) We can see that this data object holds 4 attributes: (1) The edge_index property holds the information about the graph connectivity , i.e. , a tuple of source and destination node indices for each edge. PyG further refers to (2) node features as x (each of the 34 nodes is assigned a 34-dim feature vector), and to (3) node labels as y (each node is assigned to exactly one class). (4) There also exists an additional attribute called train_mask , which describes for which nodes we already know their community assigments. In total, we are only aware of the ground-truth labels of 4 nodes (one for each community), and the task is to infer the community assignment for the remaining nodes. The data object also provides some utility functions to infer some basic properties of the underlying graph. For example, we can easily infer whether there exists isolated nodes in the graph ( i.e. there exists no edge to any node), whether the graph contains self-loops ( i.e. , $(v, v) \\in \\mathcal{E}$), or whether the graph is undirected ( i.e. , for each edge $(v, w) \\in \\mathcal{E}$ there also exists the edge $(w, v) \\in \\mathcal{E}$). from IPython.display import Javascript # Restrict height of output cell. display ( Javascript ( '''google.colab.output.setIframeHeight(0, true, {maxHeight: 300} )''' )) edge_index = data . edge_index print ( edge_index . t ()) var element = $('#5f578120-a8b7-4436-ba7d-0b6fb3047cb7'); google.colab.output.setIframeHeight(0, true, {maxHeight: 300}) tensor([[ 0, 1], [ 0, 2], [ 0, 3], [ 0, 4], [ 0, 5], [ 0, 6], [ 0, 7], [ 0, 8], [ 0, 10], [ 0, 11], [ 0, 12], [ 0, 13], [ 0, 17], [ 0, 19], [ 0, 21], [ 0, 31], [ 1, 0], [ 1, 2], [ 1, 3], [ 1, 7], [ 1, 13], [ 1, 17], [ 1, 19], [ 1, 21], [ 1, 30], [ 2, 0], [ 2, 1], [ 2, 3], [ 2, 7], [ 2, 8], [ 2, 9], [ 2, 13], [ 2, 27], [ 2, 28], [ 2, 32], [ 3, 0], [ 3, 1], [ 3, 2], [ 3, 7], [ 3, 12], [ 3, 13], [ 4, 0], [ 4, 6], [ 4, 10], [ 5, 0], [ 5, 6], [ 5, 10], [ 5, 16], [ 6, 0], [ 6, 4], [ 6, 5], [ 6, 16], [ 7, 0], [ 7, 1], [ 7, 2], [ 7, 3], [ 8, 0], [ 8, 2], [ 8, 30], [ 8, 32], [ 8, 33], [ 9, 2], [ 9, 33], [10, 0], [10, 4], [10, 5], [11, 0], [12, 0], [12, 3], [13, 0], [13, 1], [13, 2], [13, 3], [13, 33], [14, 32], [14, 33], [15, 32], [15, 33], [16, 5], [16, 6], [17, 0], [17, 1], [18, 32], [18, 33], [19, 0], [19, 1], [19, 33], [20, 32], [20, 33], [21, 0], [21, 1], [22, 32], [22, 33], [23, 25], [23, 27], [23, 29], [23, 32], [23, 33], [24, 25], [24, 27], [24, 31], [25, 23], [25, 24], [25, 31], [26, 29], [26, 33], [27, 2], [27, 23], [27, 24], [27, 33], [28, 2], [28, 31], [28, 33], [29, 23], [29, 26], [29, 32], [29, 33], [30, 1], [30, 8], [30, 32], [30, 33], [31, 0], [31, 24], [31, 25], [31, 28], [31, 32], [31, 33], [32, 2], [32, 8], [32, 14], [32, 15], [32, 18], [32, 20], [32, 22], [32, 23], [32, 29], [32, 30], [32, 31], [32, 33], [33, 8], [33, 9], [33, 13], [33, 14], [33, 15], [33, 18], [33, 19], [33, 20], [33, 22], [33, 23], [33, 26], [33, 27], [33, 28], [33, 29], [33, 30], [33, 31], [33, 32]])","title":"Data"},{"location":"ML_with_Graphs/CS224W_Colab_0/#edge-index","text":"By printing edge_index , we can further understand how PyG represents graph connectivity internally. We can see that for each edge, edge_index holds a tuple of two node indices, where the first value describes the node index of the source node and the second value describes the node index of the destination node of an edge. This representation is known as the COO format (coordinate format) commonly used for representing sparse matrices. Instead of holding the adjacency information in a dense representation $\\mathbf{A} \\in { 0, 1 }^{|\\mathcal{V}| \\times |\\mathcal{V}|}$, PyG represents graphs sparsely, which refers to only holding the coordinates/values for which entries in $\\mathbf{A}$ are non-zero. We can further visualize the graph by converting it to the networkx library format, which implements, in addition to graph manipulation functionalities, powerful tools for visualization: from torch_geometric.utils import to_networkx G = to_networkx ( data , to_undirected = True ) visualize ( G , color = data . y )","title":"Edge Index"},{"location":"ML_with_Graphs/CS224W_Colab_0/#implementing-graph-neural-networks","text":"After learning about PyG's data handling, it's time to implement our first Graph Neural Network! For this, we will use one of the most simple GNN operators, the GCN layer ( Kipf et al. (2017) ). PyG implements this layer via GCNConv , which can be executed by passing in the node feature representation x and the COO graph connectivity representation edge_index . With this, we are ready to create our first Graph Neural Network by defining our network architecture in a torch.nn.Module class: import torch from torch.nn import Linear from torch_geometric.nn import GCNConv class GCN ( torch . nn . Module ): def __init__ ( self ): super ( GCN , self ) . __init__ () torch . manual_seed ( 12345 ) self . conv1 = GCNConv ( dataset . num_features , 4 ) self . conv2 = GCNConv ( 4 , 4 ) self . conv3 = GCNConv ( 4 , 2 ) self . classifier = Linear ( 2 , dataset . num_classes ) def forward ( self , x , edge_index ): h = self . conv1 ( x , edge_index ) h = h . tanh () h = self . conv2 ( h , edge_index ) h = h . tanh () h = self . conv3 ( h , edge_index ) h = h . tanh () # Final GNN embedding space. # Apply a final (linear) classifier. out = self . classifier ( h ) return out , h model = GCN () print ( model ) GCN( (conv1): GCNConv(34, 4) (conv2): GCNConv(4, 4) (conv3): GCNConv(4, 2) (classifier): Linear(in_features=2, out_features=4, bias=True) ) Here, we first initialize all of our building blocks in __init__ and define the computation flow of our network in forward . We first define and stack three graph convolution layers , which corresponds to aggregating 3-hop neighborhood information around each node (all nodes up to 3 \"hops\" away). In addition, the GCNConv layers reduce the node feature dimensionality to $2$, i.e. , $34 \\rightarrow 4 \\rightarrow 4 \\rightarrow 2$. Each GCNConv layer is enhanced by a tanh non-linearity. After that, we apply a single linear transformation ( torch.nn.Linear ) that acts as a classifier to map our nodes to 1 out of the 4 classes/communities. We return both the output of the final classifier as well as the final node embeddings produced by our GNN. We proceed to initialize our final model via GCN() , and printing our model produces a summary of all its used sub-modules. model = GCN () _ , h = model ( data . x , data . edge_index ) print ( f 'Embedding shape: { list ( h . shape ) } ' ) visualize ( h , color = data . y ) Embedding shape: [34, 2] Remarkably, even before training the weights of our model, the model produces an embedding of nodes that closely resembles the community-structure of the graph. Nodes of the same color (community) are already closely clustered together in the embedding space, although the weights of our model are initialized completely at random and we have not yet performed any training so far! This leads to the conclusion that GNNs introduce a strong inductive bias, leading to similar embeddings for nodes that are close to each other in the input graph.","title":"Implementing Graph Neural Networks"},{"location":"ML_with_Graphs/CS224W_Colab_0/#training-on-the-karate-club-network","text":"But can we do better? Let's look at an example on how to train our network parameters based on the knowledge of the community assignments of 4 nodes in the graph (one for each community): Since everything in our model is differentiable and parameterized, we can add some labels, train the model and observe how the embeddings react. Here, we make use of a semi-supervised or transductive learning procedure: We simply train against one node per class, but are allowed to make use of the complete input graph data. Training our model is very similar to any other PyTorch model. In addition to defining our network architecture, we define a loss critertion (here, CrossEntropyLoss ) and initialize a stochastic gradient optimizer (here, Adam ). After that, we perform multiple rounds of optimization, where each round consists of a forward and backward pass to compute the gradients of our model parameters w.r.t. to the loss derived from the forward pass. If you are not new to PyTorch, this scheme should appear familar to you. Otherwise, the PyTorch docs provide a good introduction on how to train a neural network in PyTorch . Note that our semi-supervised learning scenario is achieved by the following line: loss = criterion(out[data.train_mask], data.y[data.train_mask]) While we compute node embeddings for all of our nodes, we only make use of the training nodes for computing the loss . Here, this is implemented by filtering the output of the classifier out and ground-truth labels data.y to only contain the nodes in the train_mask . Let us now start training and see how our node embeddings evolve over time (best experienced by explicitely running the code): import time from IPython.display import Javascript # Restrict height of output cell. display ( Javascript ( '''google.colab.output.setIframeHeight(0, true, {maxHeight: 430} )''' )) model = GCN () criterion = torch . nn . CrossEntropyLoss () # Define loss criterion. optimizer = torch . optim . Adam ( model . parameters (), lr = 0.01 ) # Define optimizer. def train ( data ): optimizer . zero_grad () # Clear gradients. out , h = model ( data . x , data . edge_index ) # Perform a single forward pass. loss = criterion ( out [ data . train_mask ], data . y [ data . train_mask ]) # Compute the loss solely based on the training nodes. loss . backward () # Derive gradients. optimizer . step () # Update parameters based on gradients. return loss , h for epoch in range ( 401 ): loss , h = train ( data ) # Visualize the node embeddings every 10 epochs if epoch % 10 == 0 : visualize ( h , color = data . y , epoch = epoch , loss = loss ) time . sleep ( 0.3 ) var element = $('#7fb53f8a-bbaa-4cb9-b7dc-1fc52234e214'); google.colab.output.setIframeHeight(0, true, {maxHeight: 430}) As one can see, our 3-layer GCN model manages to linearly separating the communities and classifying most of the nodes correctly. Furthermore, we did this all with a few lines of code, thanks to the PyTorch Geometric library which helped us out with data handling and GNN implementations.","title":"Training on the Karate Club Network"},{"location":"ML_with_Graphs/CS224W_Colab_0/#documentation_1","text":"You can explore more PyG functions through its documentation .","title":"Documentation"},{"location":"MacOS/Flush_DNS_Cache/","text":"Flush DNS Cache on Mac OS X 12 (Sierra) and Later \u00b6 sudo killall -HUP mDNSResponder ; \\ sudo killall mDNSResponderHelper ; \\ sudo dscacheutil -flushcache","title":"Flush DNS Cache on Mac OS"},{"location":"MacOS/Flush_DNS_Cache/#flush-dns-cache-on-mac-os-x-12-sierra-and-later","text":"sudo killall -HUP mDNSResponder ; \\ sudo killall mDNSResponderHelper ; \\ sudo dscacheutil -flushcache","title":"Flush DNS Cache on Mac OS X 12 (Sierra) and Later"},{"location":"Network_Security/ARP_Spoofing/","text":"Kali Linux ARP Spoofing \u00b6 In computer networking, ARP spoofing , ARP cache poisoning, or ARP poison routing, is a technique by which an attacker sends (spoofed) Address Resolution Protocol (ARP) messages onto a local area network. ARP \u00b6 The Address Resolution Protocol ) (ARP) is a communication protocol used for discovering the link layer address, such as a MAC address, associated with a given internet layer address, typically an IPv4 address. Check ARP on Linux/Mac/Win. arp -a Find Default Gateway \u00b6 Check your default gateway: ip route Scan the Network \u00b6 -r is for range netdiscover -r 10 .0.2.0/24 ARP Spoofing \u00b6 arpspoof -i INTERFACE -t VICTIM_IP GATEWAY_IP arpspoof -i INTERFACE -t GATEWAY_IP VICTIM_IP arpspoof -i eth0 -t 10 .0.2.15 10 .0.2.1 arpspoof -i eth0 -t 10 .0.2.1 10 .0.2.15 Make sure port forwarding is enabled, as described below. Check port forwarding \u00b6 sysctl net.ipv4.ip_forward Enable port forwarding \u00b6 sysctl -w net.ipv4.ip_forward = 1 OR, if that doesn't work: echo 1 > /proc/sys/net/ipv4/ip_forward","title":"Kali Linux ARP Spoofing"},{"location":"Network_Security/ARP_Spoofing/#kali-linux-arp-spoofing","text":"In computer networking, ARP spoofing , ARP cache poisoning, or ARP poison routing, is a technique by which an attacker sends (spoofed) Address Resolution Protocol (ARP) messages onto a local area network.","title":"Kali Linux ARP Spoofing"},{"location":"Network_Security/ARP_Spoofing/#arp","text":"The Address Resolution Protocol ) (ARP) is a communication protocol used for discovering the link layer address, such as a MAC address, associated with a given internet layer address, typically an IPv4 address. Check ARP on Linux/Mac/Win. arp -a","title":"ARP"},{"location":"Network_Security/ARP_Spoofing/#find-default-gateway","text":"Check your default gateway: ip route","title":"Find Default Gateway"},{"location":"Network_Security/ARP_Spoofing/#scan-the-network","text":"-r is for range netdiscover -r 10 .0.2.0/24","title":"Scan the Network"},{"location":"Network_Security/ARP_Spoofing/#arp-spoofing","text":"arpspoof -i INTERFACE -t VICTIM_IP GATEWAY_IP arpspoof -i INTERFACE -t GATEWAY_IP VICTIM_IP arpspoof -i eth0 -t 10 .0.2.15 10 .0.2.1 arpspoof -i eth0 -t 10 .0.2.1 10 .0.2.15 Make sure port forwarding is enabled, as described below.","title":"ARP Spoofing"},{"location":"Network_Security/ARP_Spoofing/#check-port-forwarding","text":"sysctl net.ipv4.ip_forward","title":"Check port forwarding"},{"location":"Network_Security/ARP_Spoofing/#enable-port-forwarding","text":"sysctl -w net.ipv4.ip_forward = 1 OR, if that doesn't work: echo 1 > /proc/sys/net/ipv4/ip_forward","title":"Enable port forwarding"},{"location":"PySpark/Install_With_Docker_Compose/","text":"PySpark Cluster Setup with Docker Compose \u00b6 Download 1 Bitnami docker-compose file. curl -LO https://raw.githubusercontent.com/bitnami/bitnami-docker-spark/master/docker-compose.yml Start Cluster \u00b6 docker-compose up Check Spark Dashboard \u00b6 Spark UI Bitnami Spark docker-compose \u21a9","title":"PySpark Cluster Setup with Docker Compose"},{"location":"PySpark/Install_With_Docker_Compose/#pyspark-cluster-setup-with-docker-compose","text":"Download 1 Bitnami docker-compose file. curl -LO https://raw.githubusercontent.com/bitnami/bitnami-docker-spark/master/docker-compose.yml","title":"PySpark Cluster Setup with Docker Compose"},{"location":"PySpark/Install_With_Docker_Compose/#start-cluster","text":"docker-compose up","title":"Start Cluster"},{"location":"PySpark/Install_With_Docker_Compose/#check-spark-dashboard","text":"Spark UI Bitnami Spark docker-compose \u21a9","title":"Check Spark Dashboard"},{"location":"PyTorch/gradients/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); # PyTorch Gradients This section covers the PyTorch autograd implementation of gradient descent. Tools include: * torch.autograd.backward() * torch.autograd.grad() Before continuing in this section, be sure to watch the theory lectures to understand the following concepts: * Error functions (step and sigmoid) * One-hot encoding * Maximum likelihood * Cross entropy (including multi-class cross entropy) * Back propagation (backprop) Additional Resources: \u00b6 PyTorch Notes: Autograd mechanics Autograd - Automatic Differentiation \u00b6 In previous sections we created tensors and performed a variety of operations on them, but we did nothing to store the sequence of operations, or to apply the derivative of a completed function. In this section we'll introduce the concept of the dynamic computational graph which is comprised of all the Tensor objects in the network, as well as the Functions used to create them. Note that only the input Tensors we create ourselves will not have associated Function objects. The PyTorch autograd package provides automatic differentiation for all operations on Tensors. This is because operations become attributes of the tensors themselves. When a Tensor's .requires_grad attribute is set to True, it starts to track all operations on it. When an operation finishes you can call .backward() and have all the gradients computed automatically. The gradient for a tensor will be accumulated into its .grad attribute. Let's see this in practice. Back-propagation on one step \u00b6 We'll start by applying a single polynomial function $y = f(x)$ to tensor $x$. Then we'll backprop and print the gradient $\\frac {dy} {dx}$. $\\begin{split}Function:\\quad y &= 2x^4 + x^3 + 3x^2 + 5x + 1 \\ Derivative:\\quad y' &= 8x^3 + 3x^2 + 6x + 5\\end{split}$ Step 1. Perform standard imports \u00b6 import torch Step 2. Create a tensor with requires_grad set to True \u00b6 This sets up computational tracking on the tensor. x = torch . tensor ( 2.0 , requires_grad = True ) Step 3. Define a function \u00b6 y = 2 * x ** 4 + x ** 3 + 3 * x ** 2 + 5 * x + 1 print ( y ) tensor(63., grad_fn=<AddBackward0>) Since $y$ was created as a result of an operation, it has an associated gradient function accessible as y.grad_fn The calculation of $y$ is done as: $\\quad y=2(2)^4+(2)^3+3(2)^2+5(2)+1 = 32+8+12+10+1 = 63$ This is the value of $y$ when $x=2$. Step 4. Backprop \u00b6 y . backward () Step 5. Display the resulting gradient \u00b6 print ( x . grad ) tensor(93.) Note that x.grad is an attribute of tensor $x$, so we don't use parentheses. The computation is the result of $\\quad y'=8(2)^3+3(2)^2+6(2)+5 = 64+12+12+5 = 93$ This is the slope of the polynomial at the point $(2,63)$. Back-propagation on multiple steps \u00b6 Now let's do something more complex, involving layers $y$ and $z$ between $x$ and our output layer $out$. 1. Create a tensor \u00b6 x = torch . tensor ([[ 1. , 2 , 3 ],[ 3 , 2 , 1 ]], requires_grad = True ) print ( x ) tensor([[1., 2., 3.], [3., 2., 1.]], requires_grad=True) 2. Create the first layer with $y = 3x+2$ \u00b6 y = 3 * x + 2 print ( y ) tensor([[ 5., 8., 11.], [11., 8., 5.]], grad_fn=<AddBackward0>) 3. Create the second layer with $z = 2y^2$ \u00b6 z = 2 * y ** 2 print ( z ) tensor([[ 50., 128., 242.], [242., 128., 50.]], grad_fn=<MulBackward0>) 4. Set the output to be the matrix mean \u00b6 out = z . mean () print ( out ) tensor(140., grad_fn=<MeanBackward1>) 5. Now perform back-propagation to find the gradient of x w.r.t out \u00b6 (If you haven't seen it before, w.r.t. is an abbreviation of with respect to ) out . backward () print ( x . grad ) tensor([[10., 16., 22.], [22., 16., 10.]]) You should see a 2x3 matrix. If we call the final out tensor \"$o$\", we can calculate the partial derivative of $o$ with respect to $x_i$ as follows: $o = \\frac {1} {6}\\sum_{i=1}^{6} z_i$ $z_i = 2(y_i)^2 = 2(3x_i+2)^2$ To solve the derivative of $z_i$ we use the chain rule , where the derivative of $f(g(x)) = f'(g(x))g'(x)$ In this case $\\begin{split} f(g(x)) &= 2(g(x))^2, \\quad &f'(g(x)) = 4g(x) \\ g(x) &= 3x+2, &g'(x) = 3 \\ \\frac {dz} {dx} &= 4g(x)\\times 3 &= 12(3x+2) \\end{split}$ Therefore, $\\frac{\\partial o}{\\partial x_i} = \\frac{1}{6}\\times 12(3x+2)$ $\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = 2(3(1)+2) = 10$ $\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=2} = 2(3(2)+2) = 16$ $\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=3} = 2(3(3)+2) = 22$ Turn off tracking \u00b6 There may be times when we don't want or need to track the computational history. You can reset a tensor's requires_grad attribute in-place using .requires_grad_(True) (or False) as needed. When performing evaluations, it's often helpful to wrap a set of operations in with torch.no_grad(): A less-used method is to run .detach() on a tensor to prevent future computations from being tracked. This can be handy when cloning a tensor. A NOTE ABOUT TENSORS AND VARIABLES: Prior to PyTorch v0.4.0 (April 2018) Tensors ( torch.Tensor ) only held data, and tracking history was reserved for the Variable wrapper ( torch.autograd.Variable ). Since v0.4.0 tensors and variables have merged, and tracking functionality is now available through the requires_grad=True flag.","title":"Gradients"},{"location":"PyTorch/gradients/#additional-resources","text":"PyTorch Notes: Autograd mechanics","title":"Additional Resources:"},{"location":"PyTorch/gradients/#autograd-automatic-differentiation","text":"In previous sections we created tensors and performed a variety of operations on them, but we did nothing to store the sequence of operations, or to apply the derivative of a completed function. In this section we'll introduce the concept of the dynamic computational graph which is comprised of all the Tensor objects in the network, as well as the Functions used to create them. Note that only the input Tensors we create ourselves will not have associated Function objects. The PyTorch autograd package provides automatic differentiation for all operations on Tensors. This is because operations become attributes of the tensors themselves. When a Tensor's .requires_grad attribute is set to True, it starts to track all operations on it. When an operation finishes you can call .backward() and have all the gradients computed automatically. The gradient for a tensor will be accumulated into its .grad attribute. Let's see this in practice.","title":"Autograd - Automatic Differentiation"},{"location":"PyTorch/gradients/#back-propagation-on-one-step","text":"We'll start by applying a single polynomial function $y = f(x)$ to tensor $x$. Then we'll backprop and print the gradient $\\frac {dy} {dx}$. $\\begin{split}Function:\\quad y &= 2x^4 + x^3 + 3x^2 + 5x + 1 \\ Derivative:\\quad y' &= 8x^3 + 3x^2 + 6x + 5\\end{split}$","title":"Back-propagation on one step"},{"location":"PyTorch/gradients/#step-1-perform-standard-imports","text":"import torch","title":"Step 1. Perform standard imports"},{"location":"PyTorch/gradients/#step-2-create-a-tensor-with-requires_grad-set-to-true","text":"This sets up computational tracking on the tensor. x = torch . tensor ( 2.0 , requires_grad = True )","title":"Step 2. Create a tensor with requires_grad set to True"},{"location":"PyTorch/gradients/#step-3-define-a-function","text":"y = 2 * x ** 4 + x ** 3 + 3 * x ** 2 + 5 * x + 1 print ( y ) tensor(63., grad_fn=<AddBackward0>) Since $y$ was created as a result of an operation, it has an associated gradient function accessible as y.grad_fn The calculation of $y$ is done as: $\\quad y=2(2)^4+(2)^3+3(2)^2+5(2)+1 = 32+8+12+10+1 = 63$ This is the value of $y$ when $x=2$.","title":"Step 3. Define a function"},{"location":"PyTorch/gradients/#step-4-backprop","text":"y . backward ()","title":"Step 4. Backprop"},{"location":"PyTorch/gradients/#step-5-display-the-resulting-gradient","text":"print ( x . grad ) tensor(93.) Note that x.grad is an attribute of tensor $x$, so we don't use parentheses. The computation is the result of $\\quad y'=8(2)^3+3(2)^2+6(2)+5 = 64+12+12+5 = 93$ This is the slope of the polynomial at the point $(2,63)$.","title":"Step 5. Display the resulting gradient"},{"location":"PyTorch/gradients/#back-propagation-on-multiple-steps","text":"Now let's do something more complex, involving layers $y$ and $z$ between $x$ and our output layer $out$.","title":"Back-propagation on multiple steps"},{"location":"PyTorch/gradients/#1-create-a-tensor","text":"x = torch . tensor ([[ 1. , 2 , 3 ],[ 3 , 2 , 1 ]], requires_grad = True ) print ( x ) tensor([[1., 2., 3.], [3., 2., 1.]], requires_grad=True)","title":"1. Create a tensor"},{"location":"PyTorch/gradients/#2-create-the-first-layer-with-y-3x2","text":"y = 3 * x + 2 print ( y ) tensor([[ 5., 8., 11.], [11., 8., 5.]], grad_fn=<AddBackward0>)","title":"2. Create the first layer with $y = 3x+2$"},{"location":"PyTorch/gradients/#3-create-the-second-layer-with-z-2y2","text":"z = 2 * y ** 2 print ( z ) tensor([[ 50., 128., 242.], [242., 128., 50.]], grad_fn=<MulBackward0>)","title":"3. Create the second layer with $z = 2y^2$"},{"location":"PyTorch/gradients/#4-set-the-output-to-be-the-matrix-mean","text":"out = z . mean () print ( out ) tensor(140., grad_fn=<MeanBackward1>)","title":"4. Set the output to be the matrix mean"},{"location":"PyTorch/gradients/#5-now-perform-back-propagation-to-find-the-gradient-of-x-wrt-out","text":"(If you haven't seen it before, w.r.t. is an abbreviation of with respect to ) out . backward () print ( x . grad ) tensor([[10., 16., 22.], [22., 16., 10.]]) You should see a 2x3 matrix. If we call the final out tensor \"$o$\", we can calculate the partial derivative of $o$ with respect to $x_i$ as follows: $o = \\frac {1} {6}\\sum_{i=1}^{6} z_i$ $z_i = 2(y_i)^2 = 2(3x_i+2)^2$ To solve the derivative of $z_i$ we use the chain rule , where the derivative of $f(g(x)) = f'(g(x))g'(x)$ In this case $\\begin{split} f(g(x)) &= 2(g(x))^2, \\quad &f'(g(x)) = 4g(x) \\ g(x) &= 3x+2, &g'(x) = 3 \\ \\frac {dz} {dx} &= 4g(x)\\times 3 &= 12(3x+2) \\end{split}$ Therefore, $\\frac{\\partial o}{\\partial x_i} = \\frac{1}{6}\\times 12(3x+2)$ $\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = 2(3(1)+2) = 10$ $\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=2} = 2(3(2)+2) = 16$ $\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=3} = 2(3(3)+2) = 22$","title":"5. Now perform back-propagation to find the gradient of x w.r.t out"},{"location":"PyTorch/gradients/#turn-off-tracking","text":"There may be times when we don't want or need to track the computational history. You can reset a tensor's requires_grad attribute in-place using .requires_grad_(True) (or False) as needed. When performing evaluations, it's often helpful to wrap a set of operations in with torch.no_grad(): A less-used method is to run .detach() on a tensor to prevent future computations from being tracked. This can be handy when cloning a tensor. A NOTE ABOUT TENSORS AND VARIABLES: Prior to PyTorch v0.4.0 (April 2018) Tensors ( torch.Tensor ) only held data, and tracking history was reserved for the Variable wrapper ( torch.autograd.Variable ). Since v0.4.0 tensors and variables have merged, and tracking functionality is now available through the requires_grad=True flag.","title":"Turn off tracking"},{"location":"PyTorch/tensor_basics/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); PyTorch Tensor Basics \u00b6 This section covers: * Converting NumPy arrays to PyTorch tensors * Creating tensors from scratch Perform standard imports \u00b6 import torch import numpy as np Confirm you're using PyTorch version 1.1.0+ torch . __version__ '1.5.0+cu101' Converting NumPy arrays to PyTorch tensors \u00b6 A torch.Tensor is a multi-dimensional matrix containing elements of a single data type. Calculations between tensors can only happen if the tensors share the same dtype. In some cases tensors are used as a replacement for NumPy to use the power of GPUs (more on this later). arr = np . array ([ 1 , 2 , 3 , 4 , 5 ]) print ( arr ) print ( arr . dtype ) print ( type ( arr )) [1 2 3 4 5] int64 <class 'numpy.ndarray'> x = torch . from_numpy ( arr ) # Equivalent to x = torch.as_tensor(arr) print ( x ) tensor([1, 2, 3, 4, 5]) # Print the type of data held by the tensor print ( x . dtype ) torch.int64 # Print the tensor object type print ( type ( x )) print ( x . type ()) # this is more specific! <class 'torch.Tensor'> torch.LongTensor arr2 = np . arange ( 0. , 12. ) . reshape ( 4 , 3 ) print ( arr2 ) [[ 0. 1. 2.] [ 3. 4. 5.] [ 6. 7. 8.] [ 9. 10. 11.]] x2 = torch . from_numpy ( arr2 ) print ( x2 ) print ( x2 . type ()) tensor([[ 0., 1., 2.], [ 3., 4., 5.], [ 6., 7., 8.], [ 9., 10., 11.]], dtype=torch.float64) torch.DoubleTensor Here torch.DoubleTensor refers to 64-bit floating point data. Tensor Datatypes \u00b6 TYPE NAME EQUIVALENT TENSOR TYPE 32-bit integer (signed) torch.int32 torch.int IntTensor 64-bit integer (signed) torch.int64 torch.long LongTensor 16-bit integer (signed) torch.int16 torch.short ShortTensor 32-bit floating point torch.float32 torch.float FloatTensor 64-bit floating point torch.float64 torch.double DoubleTensor 16-bit floating point torch.float16 torch.half HalfTensor 8-bit integer (signed) torch.int8 CharTensor 8-bit integer (unsigned) torch.uint8 ByteTensor Copying vs. sharing \u00b6 torch.from_numpy() torch.as_tensor() torch.tensor() There are a number of different functions available for creating tensors . When using torch.from_numpy() and torch.as_tensor() , the PyTorch tensor and the source NumPy array share the same memory. This means that changes to one affect the other. However, the torch.tensor() function always makes a copy. # Using torch.from_numpy(), shares same memory arr = np . arange ( 0 , 5 ) t = torch . from_numpy ( arr ) print ( t ) tensor([0, 1, 2, 3, 4]) arr [ 2 ] = 77 print ( t ) tensor([ 0, 1, 77, 3, 4]) # Using torch.tensor(), makes a copy arr = np . arange ( 0 , 5 ) t = torch . tensor ( arr ) print ( t ) tensor([0, 1, 2, 3, 4]) arr [ 2 ] = 77 print ( t ) tensor([0, 1, 2, 3, 4]) Class constructors \u00b6 torch.Tensor() torch.FloatTensor() torch.LongTensor() , etc. There's a subtle difference between using the factory function torch.tensor(data) and the class constructor torch.Tensor(data) . The factory function determines the dtype from the incoming data, or from a passed-in dtype argument. The class constructor torch.Tensor() is simply an alias for torch.FloatTensor(data) . Consider the following: data = np . array ([ 1 , 2 , 3 ]) a = torch . Tensor ( data ) # Equivalent to cc = torch.FloatTensor(data) print ( a , a . type ()) tensor([1., 2., 3.]) torch.FloatTensor b = torch . tensor ( data ) print ( b , b . type ()) tensor([1, 2, 3]) torch.LongTensor c = torch . tensor ( data , dtype = torch . long ) print ( c , c . type ()) tensor([1, 2, 3]) torch.LongTensor Creating tensors from scratch \u00b6 Uninitialized tensors with .empty() \u00b6 torch.empty() returns an uninitialized tensor. Essentially a block of memory is allocated according to the size of the tensor, and any values already sitting in the block are returned. This is similar to the behavior of numpy.empty() . x = torch . empty ( 4 , 3 ) print ( x ) tensor([[4.4866e-36, 0.0000e+00, 3.3631e-44], [0.0000e+00, nan, 0.0000e+00], [1.1578e+27, 1.1362e+30, 7.1547e+22], [4.5828e+30, 1.2121e+04, 7.1846e+22]]) Initialized tensors with .zeros() and .ones() \u00b6 torch.zeros(size) torch.ones(size) It's a good idea to pass in the intended dtype. x = torch . zeros ( 4 , 3 , dtype = torch . int64 ) print ( x ) tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) Tensors from ranges \u00b6 torch.arange(start,end,step) torch.linspace(start,end,steps) Note that with .arange() , end is exclusive, while with linspace() , end is inclusive. x = torch . arange ( 0 , 18 , 2 ) . reshape ( 3 , 3 ) print ( x ) tensor([[ 0, 2, 4], [ 6, 8, 10], [12, 14, 16]]) x = torch . linspace ( 0 , 18 , 12 ) . reshape ( 3 , 4 ) print ( x ) tensor([[ 0.0000, 1.6364, 3.2727, 4.9091], [ 6.5455, 8.1818, 9.8182, 11.4545], [13.0909, 14.7273, 16.3636, 18.0000]]) Tensors from data \u00b6 torch.tensor() will choose the dtype based on incoming data: x = torch . tensor ([ 1 , 2 , 3 , 4 ]) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([1, 2, 3, 4]) torch.int64 torch.LongTensor # Converting type x = x . type ( torch . int16 ) print ( x . dtype ) print ( x . type ()) torch.int16 torch.ShortTensor Alternatively you can set the type by the tensor method used. For a list of tensor types visit https://pytorch.org/docs/stable/tensors.html x = torch . FloatTensor ([ 5 , 6 , 7 ]) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([5., 6., 7.]) torch.float32 torch.FloatTensor You can also pass the dtype in as an argument. For a list of dtypes visit https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype x = torch . tensor ([ 8 , 9 , - 3 ], dtype = torch . int ) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([ 8, 9, -3], dtype=torch.int32) torch.int32 torch.IntTensor Changing the dtype of existing tensors \u00b6 Don't be tempted to use x = torch.tensor(x, dtype=torch.type) as it will raise an error about improper use of tensor cloning. Instead, use the tensor .type() method. print ( 'Old:' , x . type ()) x = x . type ( torch . int64 ) print ( 'New:' , x . type ()) Old: torch.IntTensor New: torch.LongTensor Random number tensors \u00b6 torch.rand(size) returns random samples from a uniform distribution over [0, 1) torch.randn(size) returns samples from the \"standard normal\" distribution [\u03c3 = 1] Unlike rand which is uniform, values closer to zero are more likely to appear. torch.randint(low,high,size) returns random integers from low (inclusive) to high (exclusive) x = torch . rand ( 4 , 3 ) print ( x ) tensor([[0.6682, 0.3914, 0.5425], [0.2000, 0.3056, 0.9103], [0.7039, 0.5021, 0.9170], [0.4305, 0.7270, 0.6577]]) x = torch . randn ( 4 , 3 ) print ( x ) tensor([[ 0.4623, -0.2561, -0.5399], [-0.6609, -0.6707, 0.6866], [-0.9742, -0.3833, 0.1253], [ 0.1251, -0.7600, -1.8088]]) x = torch . randint ( 0 , 5 , ( 4 , 3 )) print ( x ) tensor([[0, 2, 2], [3, 1, 2], [3, 2, 1], [2, 0, 4]]) Random number tensors that follow the input size \u00b6 torch.rand_like(input) torch.randn_like(input) torch.randint_like(input,low,high) these return random number tensors with the same size as input x = torch . zeros ( 2 , 5 ) print ( x ) tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) x2 = torch . randn_like ( x ) print ( x2 ) tensor([[-0.5288, 0.5442, 1.8976, 0.5154, 2.7177], [-1.7115, -1.4005, 0.2681, -0.0782, -0.6214]]) The same syntax can be used with torch.zeros_like(input) torch.ones_like(input) x3 = torch . ones_like ( x2 ) print ( x3 ) tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]) Setting the random seed \u00b6 torch.manual_seed(int) is used to obtain reproducible results torch . manual_seed ( 42 ) x = torch . rand ( 2 , 3 ) print ( x ) tensor([[0.8823, 0.9150, 0.3829], [0.9593, 0.3904, 0.6009]]) torch . manual_seed ( 42 ) x = torch . rand ( 2 , 3 ) print ( x ) tensor([[0.8823, 0.9150, 0.3829], [0.9593, 0.3904, 0.6009]]) Tensor attributes \u00b6 Besides dtype , we can look at other tensor attributes like shape , device and layout x . shape torch.Size([2, 3]) x . size () # equivalent to x.shape torch.Size([2, 3]) x . device device(type='cpu') PyTorch supports use of multiple devices , harnessing the power of one or more GPUs in addition to the CPU. We won't explore that here, but you should know that operations between tensors can only happen for tensors installed on the same device. x . layout torch.strided PyTorch has a class to hold the memory layout option. The default setting of strided will suit our purposes throughout the course.","title":"Tensor basics"},{"location":"PyTorch/tensor_basics/#pytorch-tensor-basics","text":"This section covers: * Converting NumPy arrays to PyTorch tensors * Creating tensors from scratch","title":"PyTorch Tensor Basics"},{"location":"PyTorch/tensor_basics/#perform-standard-imports","text":"import torch import numpy as np Confirm you're using PyTorch version 1.1.0+ torch . __version__ '1.5.0+cu101'","title":"Perform standard imports"},{"location":"PyTorch/tensor_basics/#converting-numpy-arrays-to-pytorch-tensors","text":"A torch.Tensor is a multi-dimensional matrix containing elements of a single data type. Calculations between tensors can only happen if the tensors share the same dtype. In some cases tensors are used as a replacement for NumPy to use the power of GPUs (more on this later). arr = np . array ([ 1 , 2 , 3 , 4 , 5 ]) print ( arr ) print ( arr . dtype ) print ( type ( arr )) [1 2 3 4 5] int64 <class 'numpy.ndarray'> x = torch . from_numpy ( arr ) # Equivalent to x = torch.as_tensor(arr) print ( x ) tensor([1, 2, 3, 4, 5]) # Print the type of data held by the tensor print ( x . dtype ) torch.int64 # Print the tensor object type print ( type ( x )) print ( x . type ()) # this is more specific! <class 'torch.Tensor'> torch.LongTensor arr2 = np . arange ( 0. , 12. ) . reshape ( 4 , 3 ) print ( arr2 ) [[ 0. 1. 2.] [ 3. 4. 5.] [ 6. 7. 8.] [ 9. 10. 11.]] x2 = torch . from_numpy ( arr2 ) print ( x2 ) print ( x2 . type ()) tensor([[ 0., 1., 2.], [ 3., 4., 5.], [ 6., 7., 8.], [ 9., 10., 11.]], dtype=torch.float64) torch.DoubleTensor Here torch.DoubleTensor refers to 64-bit floating point data.","title":"Converting NumPy arrays to PyTorch tensors"},{"location":"PyTorch/tensor_basics/#tensor-datatypes","text":"TYPE NAME EQUIVALENT TENSOR TYPE 32-bit integer (signed) torch.int32 torch.int IntTensor 64-bit integer (signed) torch.int64 torch.long LongTensor 16-bit integer (signed) torch.int16 torch.short ShortTensor 32-bit floating point torch.float32 torch.float FloatTensor 64-bit floating point torch.float64 torch.double DoubleTensor 16-bit floating point torch.float16 torch.half HalfTensor 8-bit integer (signed) torch.int8 CharTensor 8-bit integer (unsigned) torch.uint8 ByteTensor","title":"Tensor Datatypes"},{"location":"PyTorch/tensor_basics/#copying-vs-sharing","text":"torch.from_numpy() torch.as_tensor() torch.tensor() There are a number of different functions available for creating tensors . When using torch.from_numpy() and torch.as_tensor() , the PyTorch tensor and the source NumPy array share the same memory. This means that changes to one affect the other. However, the torch.tensor() function always makes a copy. # Using torch.from_numpy(), shares same memory arr = np . arange ( 0 , 5 ) t = torch . from_numpy ( arr ) print ( t ) tensor([0, 1, 2, 3, 4]) arr [ 2 ] = 77 print ( t ) tensor([ 0, 1, 77, 3, 4]) # Using torch.tensor(), makes a copy arr = np . arange ( 0 , 5 ) t = torch . tensor ( arr ) print ( t ) tensor([0, 1, 2, 3, 4]) arr [ 2 ] = 77 print ( t ) tensor([0, 1, 2, 3, 4])","title":"Copying vs. sharing"},{"location":"PyTorch/tensor_basics/#class-constructors","text":"torch.Tensor() torch.FloatTensor() torch.LongTensor() , etc. There's a subtle difference between using the factory function torch.tensor(data) and the class constructor torch.Tensor(data) . The factory function determines the dtype from the incoming data, or from a passed-in dtype argument. The class constructor torch.Tensor() is simply an alias for torch.FloatTensor(data) . Consider the following: data = np . array ([ 1 , 2 , 3 ]) a = torch . Tensor ( data ) # Equivalent to cc = torch.FloatTensor(data) print ( a , a . type ()) tensor([1., 2., 3.]) torch.FloatTensor b = torch . tensor ( data ) print ( b , b . type ()) tensor([1, 2, 3]) torch.LongTensor c = torch . tensor ( data , dtype = torch . long ) print ( c , c . type ()) tensor([1, 2, 3]) torch.LongTensor","title":"Class constructors"},{"location":"PyTorch/tensor_basics/#creating-tensors-from-scratch","text":"","title":"Creating tensors from scratch"},{"location":"PyTorch/tensor_basics/#uninitialized-tensors-with-empty","text":"torch.empty() returns an uninitialized tensor. Essentially a block of memory is allocated according to the size of the tensor, and any values already sitting in the block are returned. This is similar to the behavior of numpy.empty() . x = torch . empty ( 4 , 3 ) print ( x ) tensor([[4.4866e-36, 0.0000e+00, 3.3631e-44], [0.0000e+00, nan, 0.0000e+00], [1.1578e+27, 1.1362e+30, 7.1547e+22], [4.5828e+30, 1.2121e+04, 7.1846e+22]])","title":"Uninitialized tensors with .empty()"},{"location":"PyTorch/tensor_basics/#initialized-tensors-with-zeros-and-ones","text":"torch.zeros(size) torch.ones(size) It's a good idea to pass in the intended dtype. x = torch . zeros ( 4 , 3 , dtype = torch . int64 ) print ( x ) tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]])","title":"Initialized tensors with .zeros() and .ones()"},{"location":"PyTorch/tensor_basics/#tensors-from-ranges","text":"torch.arange(start,end,step) torch.linspace(start,end,steps) Note that with .arange() , end is exclusive, while with linspace() , end is inclusive. x = torch . arange ( 0 , 18 , 2 ) . reshape ( 3 , 3 ) print ( x ) tensor([[ 0, 2, 4], [ 6, 8, 10], [12, 14, 16]]) x = torch . linspace ( 0 , 18 , 12 ) . reshape ( 3 , 4 ) print ( x ) tensor([[ 0.0000, 1.6364, 3.2727, 4.9091], [ 6.5455, 8.1818, 9.8182, 11.4545], [13.0909, 14.7273, 16.3636, 18.0000]])","title":"Tensors from ranges"},{"location":"PyTorch/tensor_basics/#tensors-from-data","text":"torch.tensor() will choose the dtype based on incoming data: x = torch . tensor ([ 1 , 2 , 3 , 4 ]) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([1, 2, 3, 4]) torch.int64 torch.LongTensor # Converting type x = x . type ( torch . int16 ) print ( x . dtype ) print ( x . type ()) torch.int16 torch.ShortTensor Alternatively you can set the type by the tensor method used. For a list of tensor types visit https://pytorch.org/docs/stable/tensors.html x = torch . FloatTensor ([ 5 , 6 , 7 ]) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([5., 6., 7.]) torch.float32 torch.FloatTensor You can also pass the dtype in as an argument. For a list of dtypes visit https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype x = torch . tensor ([ 8 , 9 , - 3 ], dtype = torch . int ) print ( x ) print ( x . dtype ) print ( x . type ()) tensor([ 8, 9, -3], dtype=torch.int32) torch.int32 torch.IntTensor","title":"Tensors from data"},{"location":"PyTorch/tensor_basics/#changing-the-dtype-of-existing-tensors","text":"Don't be tempted to use x = torch.tensor(x, dtype=torch.type) as it will raise an error about improper use of tensor cloning. Instead, use the tensor .type() method. print ( 'Old:' , x . type ()) x = x . type ( torch . int64 ) print ( 'New:' , x . type ()) Old: torch.IntTensor New: torch.LongTensor","title":"Changing the dtype of existing tensors"},{"location":"PyTorch/tensor_basics/#random-number-tensors","text":"torch.rand(size) returns random samples from a uniform distribution over [0, 1) torch.randn(size) returns samples from the \"standard normal\" distribution [\u03c3 = 1] Unlike rand which is uniform, values closer to zero are more likely to appear. torch.randint(low,high,size) returns random integers from low (inclusive) to high (exclusive) x = torch . rand ( 4 , 3 ) print ( x ) tensor([[0.6682, 0.3914, 0.5425], [0.2000, 0.3056, 0.9103], [0.7039, 0.5021, 0.9170], [0.4305, 0.7270, 0.6577]]) x = torch . randn ( 4 , 3 ) print ( x ) tensor([[ 0.4623, -0.2561, -0.5399], [-0.6609, -0.6707, 0.6866], [-0.9742, -0.3833, 0.1253], [ 0.1251, -0.7600, -1.8088]]) x = torch . randint ( 0 , 5 , ( 4 , 3 )) print ( x ) tensor([[0, 2, 2], [3, 1, 2], [3, 2, 1], [2, 0, 4]])","title":"Random number tensors"},{"location":"PyTorch/tensor_basics/#random-number-tensors-that-follow-the-input-size","text":"torch.rand_like(input) torch.randn_like(input) torch.randint_like(input,low,high) these return random number tensors with the same size as input x = torch . zeros ( 2 , 5 ) print ( x ) tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) x2 = torch . randn_like ( x ) print ( x2 ) tensor([[-0.5288, 0.5442, 1.8976, 0.5154, 2.7177], [-1.7115, -1.4005, 0.2681, -0.0782, -0.6214]]) The same syntax can be used with torch.zeros_like(input) torch.ones_like(input) x3 = torch . ones_like ( x2 ) print ( x3 ) tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]])","title":"Random number tensors that follow the input size"},{"location":"PyTorch/tensor_basics/#setting-the-random-seed","text":"torch.manual_seed(int) is used to obtain reproducible results torch . manual_seed ( 42 ) x = torch . rand ( 2 , 3 ) print ( x ) tensor([[0.8823, 0.9150, 0.3829], [0.9593, 0.3904, 0.6009]]) torch . manual_seed ( 42 ) x = torch . rand ( 2 , 3 ) print ( x ) tensor([[0.8823, 0.9150, 0.3829], [0.9593, 0.3904, 0.6009]])","title":"Setting the random seed"},{"location":"PyTorch/tensor_basics/#tensor-attributes","text":"Besides dtype , we can look at other tensor attributes like shape , device and layout x . shape torch.Size([2, 3]) x . size () # equivalent to x.shape torch.Size([2, 3]) x . device device(type='cpu') PyTorch supports use of multiple devices , harnessing the power of one or more GPUs in addition to the CPU. We won't explore that here, but you should know that operations between tensors can only happen for tensors installed on the same device. x . layout torch.strided PyTorch has a class to hold the memory layout option. The default setting of strided will suit our purposes throughout the course.","title":"Tensor attributes"},{"location":"PyTorch/tensor_operations/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); PyTorch Tensor Operations \u00b6 This section covers: * Indexing and slicing * Reshaping tensors (tensor views) * Tensor arithmetic and basic operations * Dot products * Matrix multiplication * Additional, more advanced operations Perform standard imports \u00b6 import torch import numpy as np Indexing and slicing \u00b6 Extracting specific values from a tensor works just the same as with NumPy arrays Image source: http://www.scipy-lectures.org/_images/numpy_indexing.png x = torch . arange ( 6 ) . reshape ( 3 , 2 ) print ( x ) tensor([[0, 1], [2, 3], [4, 5]]) # Grabbing the right hand column values x [:, 1 ] tensor([1, 3, 5]) # Grabbing the right hand column as a (3,1) slice x [:, 1 :] tensor([[1], [3], [5]]) Reshape tensors with .view() \u00b6 view() and reshape() do essentially the same thing by returning a reshaped tensor without changing the original tensor in place. There's a good discussion of the differences here . x = torch . arange ( 12 ) print ( x ) tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) x . view ( 2 , 6 ) tensor([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) x . view ( 6 , 2 ) tensor([[ 0, 1], [ 2, 3], [ 4, 5], [ 6, 7], [ 8, 9], [10, 11]]) # x is unchanged x tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) Views reflect the most current data \u00b6 z = x . view ( 2 , 6 ) x [ 0 ] = 234 print ( z ) tensor([[234, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) Views can infer the correct size \u00b6 By passing in -1 PyTorch will infer the correct value from the given tensor # infer number of columns for given rows x . view ( 2 , - 1 ) tensor([[234, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) # infer number of rows for given columns x . view ( - 1 , 3 ) tensor([[234, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11]]) Adopt another tensor's shape with .view_as() \u00b6 view_as(input) only works with tensors that have the same number of elements. x . view_as ( z ) tensor([[234, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) Tensor Arithmetic \u00b6 Adding tensors can be performed a few different ways depending on the desired result. As a simple expression: a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( a + b ) tensor([5., 7., 9.]) As arguments passed into a torch operation: print ( torch . add ( a , b )) tensor([5., 7., 9.]) With an output tensor passed in as an argument: result = torch . empty ( 3 ) torch . add ( a , b , out = result ) # equivalent to result=torch.add(a,b) print ( result ) tensor([5., 7., 9.]) Changing a tensor in-place with _ a . add_ ( b ) # equivalent to a=torch.add(a,b) print ( a ) tensor([5., 7., 9.]) NOTE: Any operation that changes a tensor in-place is post-fixed with an underscore _. In the above example: a.add_(b) changed a . Basic Tensor Operations \u00b6 Arithmetic OPERATION FUNCTION DESCRIPTION a + b a.add(b) element wise addition a - b a.sub(b) subtraction a * b a.mul(b) multiplication a / b a.div(b) division a % b a.fmod(b) modulo (remainder after division) a b a.pow(b) power Monomial Operations OPERATION FUNCTION DESCRIPTION |a| torch.abs(a) absolute value 1/a torch.reciprocal(a) reciprocal $\\sqrt{a}$ torch.sqrt(a) square root log(a) torch.log(a) natural log e a torch.exp(a) exponential 12.34 ==> 12. torch.trunc(a) truncated integer 12.34 ==> 0.34 torch.frac(a) fractional component Trigonometry OPERATION FUNCTION DESCRIPTION sin(a) torch.sin(a) sine cos(a) torch.sin(a) cosine tan(a) torch.sin(a) tangent arcsin(a) torch.asin(a) arc sine arccos(a) torch.acos(a) arc cosine arctan(a) torch.atan(a) arc tangent sinh(a) torch.sinh(a) hyperbolic sine cosh(a) torch.cosh(a) hyperbolic cosine tanh(a) torch.tanh(a) hyperbolic tangent Summary Statistics OPERATION FUNCTION DESCRIPTION $\\sum a$ torch.sum(a) sum $\\bar a$ torch.mean(a) mean a max torch.max(a) maximum a min torch.min(a) minimum torch.max(a,b) returns a tensor of size a containing the element wise max between a and b NOTE: Most arithmetic operations require float values. Those that do work with integers return integer tensors. For example, torch.div(a,b) performs floor division (truncates the decimal) for integer types, and classic division for floats. Use the space below to experiment with different operations \u00b6 a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( torch . add ( a , b ) . sum ()) tensor(21.) Dot products \u00b6 A dot product is the sum of the products of the corresponding entries of two 1D tensors. If the tensors are both vectors, the dot product is given as: $\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d & e & f \\end{bmatrix} = ad + be + cf$ If the tensors include a column vector, then the dot product is the sum of the result of the multiplied matrices. For example: $\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d \\ e \\ f \\end{bmatrix} = ad + be + cf$ Dot products can be expressed as torch.dot(a,b) or a.dot(b) or b.dot(a) a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( a . mul ( b )) # for reference print () print ( a . dot ( b )) tensor([ 4., 10., 18.]) tensor(32.) NOTE: There's a slight difference between torch.dot() and numpy.dot() . While torch.dot() only accepts 1D arguments and returns a dot product, numpy.dot() also accepts 2D arguments and performs matrix multiplication. We show matrix multiplication below. Matrix multiplication \u00b6 2D Matrix multiplication is possible when the number of columns in tensor A matches the number of rows in tensor B . In this case, the product of tensor A with size $(x,y)$ and tensor B with size $(y,z)$ results in a tensor of size $(x,z)$ $\\begin{bmatrix} a & b & c \\\\ d & e & f \\end{bmatrix} \\;\\times\\; \\begin{bmatrix} m & n \\\\ p & q \\\\ r & s \\end{bmatrix} = \\begin{bmatrix} (am+bp+cr) & (an+bq+cs) \\\\ (dm+ep+fr) & (dn+eq+fs) \\end{bmatrix}$ Image source: https://commons.wikimedia.org/wiki/File:Matrix_multiplication_diagram_2.svg Matrix multiplication can be computed using torch.mm(a,b) or a.mm(b) or a @ b a = torch . tensor ([[ 0 , 2 , 4 ],[ 1 , 3 , 5 ]], dtype = torch . float ) b = torch . tensor ([[ 6 , 7 ],[ 8 , 9 ],[ 10 , 11 ]], dtype = torch . float ) print ( 'a: ' , a . size ()) print ( 'b: ' , b . size ()) print ( 'a x b: ' , torch . mm ( a , b ) . size ()) a: torch.Size([2, 3]) b: torch.Size([3, 2]) a x b: torch.Size([2, 2]) print ( torch . mm ( a , b )) tensor([[56., 62.], [80., 89.]]) print ( a . mm ( b )) tensor([[56., 62.], [80., 89.]]) print ( a @ b ) tensor([[56., 62.], [80., 89.]]) Matrix multiplication with broadcasting \u00b6 Matrix multiplication that involves broadcasting can be computed using torch.matmul(a,b) or a.matmul(b) or a @ b t1 = torch . randn ( 2 , 3 , 4 ) t2 = torch . randn ( 4 , 5 ) t1 tensor([[[ 0.0495, -1.2814, 0.4144, 0.3883], [-2.1511, 0.0932, 2.0666, 0.8509], [ 0.4211, -2.1292, 0.9620, -1.6141]], [[ 0.6840, -0.7749, 0.7027, 0.0369], [-0.0445, 0.4145, -0.2296, 1.2467], [ 0.2800, -1.7043, 0.2537, 0.1963]]]) t2 tensor([[ 1.9903, 0.3279, -0.2475, 0.5449, 0.0568], [-0.5038, -0.0790, -0.1920, 0.1574, -0.2723], [ 0.1912, 0.8469, -1.7464, 1.1971, 2.7874], [-0.8376, 0.5609, 0.8387, 1.5994, 0.0535]]) print ( torch . matmul ( t1 , t2 ) . size ()) torch.Size([2, 3, 5]) However, the same operation raises a RuntimeError with torch.mm() : print ( torch . mm ( t1 , t2 ) . size ()) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-46-edaac219da2b> in <module> () ----> 1 print ( torch . mm ( t1 , t2 ) . size ( ) ) RuntimeError : matrices expected, got 3D, 2D tensors at /pytorch/aten/src/TH/generic/THTensorMath.cpp:36 Advanced operations \u00b6 L2 or Euclidian Norm \u00b6 See torch.norm() The Euclidian Norm gives the vector norm of $x$ where $x=(x_1,x_2,...,x_n)$. It is calculated as ${\\displaystyle \\left|{\\boldsymbol {x}}\\right| {2}:={\\sqrt {x ^{2}+\\cdots +x_{n}^{2}}}}$ When applied to a matrix, torch.norm() returns the Frobenius norm by default. x = torch . tensor ([ 2. , 5. , 8. , 14. ]) x . norm () tensor(17.) Number of elements \u00b6 See torch.numel() Returns the number of elements in a tensor. x = torch . ones ( 3 , 7 ) x . numel () 21 This can be useful in certain calculations like Mean Squared Error: def mse(t1, t2): diff = t1 - t2 return torch.sum(diff * diff) / diff .numel()","title":"Tensor operations"},{"location":"PyTorch/tensor_operations/#pytorch-tensor-operations","text":"This section covers: * Indexing and slicing * Reshaping tensors (tensor views) * Tensor arithmetic and basic operations * Dot products * Matrix multiplication * Additional, more advanced operations","title":"PyTorch Tensor Operations"},{"location":"PyTorch/tensor_operations/#perform-standard-imports","text":"import torch import numpy as np","title":"Perform standard imports"},{"location":"PyTorch/tensor_operations/#indexing-and-slicing","text":"Extracting specific values from a tensor works just the same as with NumPy arrays Image source: http://www.scipy-lectures.org/_images/numpy_indexing.png x = torch . arange ( 6 ) . reshape ( 3 , 2 ) print ( x ) tensor([[0, 1], [2, 3], [4, 5]]) # Grabbing the right hand column values x [:, 1 ] tensor([1, 3, 5]) # Grabbing the right hand column as a (3,1) slice x [:, 1 :] tensor([[1], [3], [5]])","title":"Indexing and slicing"},{"location":"PyTorch/tensor_operations/#reshape-tensors-with-view","text":"view() and reshape() do essentially the same thing by returning a reshaped tensor without changing the original tensor in place. There's a good discussion of the differences here . x = torch . arange ( 12 ) print ( x ) tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) x . view ( 2 , 6 ) tensor([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) x . view ( 6 , 2 ) tensor([[ 0, 1], [ 2, 3], [ 4, 5], [ 6, 7], [ 8, 9], [10, 11]]) # x is unchanged x tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])","title":"Reshape tensors with .view()"},{"location":"PyTorch/tensor_operations/#views-reflect-the-most-current-data","text":"z = x . view ( 2 , 6 ) x [ 0 ] = 234 print ( z ) tensor([[234, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]])","title":"Views reflect the most current data"},{"location":"PyTorch/tensor_operations/#views-can-infer-the-correct-size","text":"By passing in -1 PyTorch will infer the correct value from the given tensor # infer number of columns for given rows x . view ( 2 , - 1 ) tensor([[234, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) # infer number of rows for given columns x . view ( - 1 , 3 ) tensor([[234, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11]])","title":"Views can infer the correct size"},{"location":"PyTorch/tensor_operations/#adopt-another-tensors-shape-with-view_as","text":"view_as(input) only works with tensors that have the same number of elements. x . view_as ( z ) tensor([[234, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]])","title":"Adopt another tensor's shape with .view_as()"},{"location":"PyTorch/tensor_operations/#tensor-arithmetic","text":"Adding tensors can be performed a few different ways depending on the desired result. As a simple expression: a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( a + b ) tensor([5., 7., 9.]) As arguments passed into a torch operation: print ( torch . add ( a , b )) tensor([5., 7., 9.]) With an output tensor passed in as an argument: result = torch . empty ( 3 ) torch . add ( a , b , out = result ) # equivalent to result=torch.add(a,b) print ( result ) tensor([5., 7., 9.]) Changing a tensor in-place with _ a . add_ ( b ) # equivalent to a=torch.add(a,b) print ( a ) tensor([5., 7., 9.]) NOTE: Any operation that changes a tensor in-place is post-fixed with an underscore _. In the above example: a.add_(b) changed a .","title":"Tensor Arithmetic"},{"location":"PyTorch/tensor_operations/#basic-tensor-operations","text":"Arithmetic OPERATION FUNCTION DESCRIPTION a + b a.add(b) element wise addition a - b a.sub(b) subtraction a * b a.mul(b) multiplication a / b a.div(b) division a % b a.fmod(b) modulo (remainder after division) a b a.pow(b) power Monomial Operations OPERATION FUNCTION DESCRIPTION |a| torch.abs(a) absolute value 1/a torch.reciprocal(a) reciprocal $\\sqrt{a}$ torch.sqrt(a) square root log(a) torch.log(a) natural log e a torch.exp(a) exponential 12.34 ==> 12. torch.trunc(a) truncated integer 12.34 ==> 0.34 torch.frac(a) fractional component Trigonometry OPERATION FUNCTION DESCRIPTION sin(a) torch.sin(a) sine cos(a) torch.sin(a) cosine tan(a) torch.sin(a) tangent arcsin(a) torch.asin(a) arc sine arccos(a) torch.acos(a) arc cosine arctan(a) torch.atan(a) arc tangent sinh(a) torch.sinh(a) hyperbolic sine cosh(a) torch.cosh(a) hyperbolic cosine tanh(a) torch.tanh(a) hyperbolic tangent Summary Statistics OPERATION FUNCTION DESCRIPTION $\\sum a$ torch.sum(a) sum $\\bar a$ torch.mean(a) mean a max torch.max(a) maximum a min torch.min(a) minimum torch.max(a,b) returns a tensor of size a containing the element wise max between a and b NOTE: Most arithmetic operations require float values. Those that do work with integers return integer tensors. For example, torch.div(a,b) performs floor division (truncates the decimal) for integer types, and classic division for floats.","title":"Basic Tensor Operations"},{"location":"PyTorch/tensor_operations/#use-the-space-below-to-experiment-with-different-operations","text":"a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( torch . add ( a , b ) . sum ()) tensor(21.)","title":"Use the space below to experiment with different operations"},{"location":"PyTorch/tensor_operations/#dot-products","text":"A dot product is the sum of the products of the corresponding entries of two 1D tensors. If the tensors are both vectors, the dot product is given as: $\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d & e & f \\end{bmatrix} = ad + be + cf$ If the tensors include a column vector, then the dot product is the sum of the result of the multiplied matrices. For example: $\\begin{bmatrix} a & b & c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d \\ e \\ f \\end{bmatrix} = ad + be + cf$ Dot products can be expressed as torch.dot(a,b) or a.dot(b) or b.dot(a) a = torch . tensor ([ 1 , 2 , 3 ], dtype = torch . float ) b = torch . tensor ([ 4 , 5 , 6 ], dtype = torch . float ) print ( a . mul ( b )) # for reference print () print ( a . dot ( b )) tensor([ 4., 10., 18.]) tensor(32.) NOTE: There's a slight difference between torch.dot() and numpy.dot() . While torch.dot() only accepts 1D arguments and returns a dot product, numpy.dot() also accepts 2D arguments and performs matrix multiplication. We show matrix multiplication below.","title":"Dot products"},{"location":"PyTorch/tensor_operations/#matrix-multiplication","text":"2D Matrix multiplication is possible when the number of columns in tensor A matches the number of rows in tensor B . In this case, the product of tensor A with size $(x,y)$ and tensor B with size $(y,z)$ results in a tensor of size $(x,z)$ $\\begin{bmatrix} a & b & c \\\\ d & e & f \\end{bmatrix} \\;\\times\\; \\begin{bmatrix} m & n \\\\ p & q \\\\ r & s \\end{bmatrix} = \\begin{bmatrix} (am+bp+cr) & (an+bq+cs) \\\\ (dm+ep+fr) & (dn+eq+fs) \\end{bmatrix}$ Image source: https://commons.wikimedia.org/wiki/File:Matrix_multiplication_diagram_2.svg Matrix multiplication can be computed using torch.mm(a,b) or a.mm(b) or a @ b a = torch . tensor ([[ 0 , 2 , 4 ],[ 1 , 3 , 5 ]], dtype = torch . float ) b = torch . tensor ([[ 6 , 7 ],[ 8 , 9 ],[ 10 , 11 ]], dtype = torch . float ) print ( 'a: ' , a . size ()) print ( 'b: ' , b . size ()) print ( 'a x b: ' , torch . mm ( a , b ) . size ()) a: torch.Size([2, 3]) b: torch.Size([3, 2]) a x b: torch.Size([2, 2]) print ( torch . mm ( a , b )) tensor([[56., 62.], [80., 89.]]) print ( a . mm ( b )) tensor([[56., 62.], [80., 89.]]) print ( a @ b ) tensor([[56., 62.], [80., 89.]])","title":"Matrix multiplication"},{"location":"PyTorch/tensor_operations/#matrix-multiplication-with-broadcasting","text":"Matrix multiplication that involves broadcasting can be computed using torch.matmul(a,b) or a.matmul(b) or a @ b t1 = torch . randn ( 2 , 3 , 4 ) t2 = torch . randn ( 4 , 5 ) t1 tensor([[[ 0.0495, -1.2814, 0.4144, 0.3883], [-2.1511, 0.0932, 2.0666, 0.8509], [ 0.4211, -2.1292, 0.9620, -1.6141]], [[ 0.6840, -0.7749, 0.7027, 0.0369], [-0.0445, 0.4145, -0.2296, 1.2467], [ 0.2800, -1.7043, 0.2537, 0.1963]]]) t2 tensor([[ 1.9903, 0.3279, -0.2475, 0.5449, 0.0568], [-0.5038, -0.0790, -0.1920, 0.1574, -0.2723], [ 0.1912, 0.8469, -1.7464, 1.1971, 2.7874], [-0.8376, 0.5609, 0.8387, 1.5994, 0.0535]]) print ( torch . matmul ( t1 , t2 ) . size ()) torch.Size([2, 3, 5]) However, the same operation raises a RuntimeError with torch.mm() : print ( torch . mm ( t1 , t2 ) . size ()) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-46-edaac219da2b> in <module> () ----> 1 print ( torch . mm ( t1 , t2 ) . size ( ) ) RuntimeError : matrices expected, got 3D, 2D tensors at /pytorch/aten/src/TH/generic/THTensorMath.cpp:36","title":"Matrix multiplication with broadcasting"},{"location":"PyTorch/tensor_operations/#advanced-operations","text":"","title":"Advanced operations"},{"location":"PyTorch/tensor_operations/#l2-or-euclidian-norm","text":"See torch.norm() The Euclidian Norm gives the vector norm of $x$ where $x=(x_1,x_2,...,x_n)$. It is calculated as ${\\displaystyle \\left|{\\boldsymbol {x}}\\right| {2}:={\\sqrt {x ^{2}+\\cdots +x_{n}^{2}}}}$ When applied to a matrix, torch.norm() returns the Frobenius norm by default. x = torch . tensor ([ 2. , 5. , 8. , 14. ]) x . norm () tensor(17.)","title":"L2 or Euclidian Norm"},{"location":"PyTorch/tensor_operations/#number-of-elements","text":"See torch.numel() Returns the number of elements in a tensor. x = torch . ones ( 3 , 7 ) x . numel () 21 This can be useful in certain calculations like Mean Squared Error: def mse(t1, t2): diff = t1 - t2 return torch.sum(diff * diff) / diff .numel()","title":"Number of elements"},{"location":"Python/Basics/ASCII/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Python ASCII Encoding \u00b6 ASCII abbreviated from American Standard Code for Information Interchange , is a character encoding standard for electronic communication. ASCII codes represent text in computers, telecommunications equipment, and other devices. ASCII Value of a Character \u00b6 Print ASCII value of character 'a'. print ( ord ( 'a' )) 97 Print ASCII value of character 'A'. print ( ord ( 'A' )) 65 Character from ASCII Value \u00b6 Print character represented by ACII value 97. print ( chr ( 97 )) a Print character represented by ACII value 65. print ( chr ( 65 )) A","title":"ASCII"},{"location":"Python/Basics/ASCII/#python-ascii-encoding","text":"ASCII abbreviated from American Standard Code for Information Interchange , is a character encoding standard for electronic communication. ASCII codes represent text in computers, telecommunications equipment, and other devices.","title":"Python ASCII Encoding"},{"location":"Python/Basics/ASCII/#ascii-value-of-a-character","text":"Print ASCII value of character 'a'. print ( ord ( 'a' )) 97 Print ASCII value of character 'A'. print ( ord ( 'A' )) 65","title":"ASCII Value of a Character"},{"location":"Python/Basics/ASCII/#character-from-ascii-value","text":"Print character represented by ACII value 97. print ( chr ( 97 )) a Print character represented by ACII value 65. print ( chr ( 65 )) A","title":"Character from ASCII Value"},{"location":"Python/Basics/ISO_8601/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); ISO 8601 Timestamp in Python \u00b6 ISO 8601 Data elements and interchange formats \u2013 Information interchange \u2013 Representation of dates and times is an international standard covering the exchange of date- and time-related data. It was issued by the International Organization for Standardization (ISO) and was first published in 1988. from datetime import datetime , timezone , tzinfo UTC Time \u00b6 # UTC time now print ( datetime . utcnow () . replace ( microsecond = 0 ) . isoformat ()) 2020-05-12T16:16:06 # UTC time now with timezone info print ( datetime . now () . astimezone ( tz = timezone . utc ) . replace ( microsecond = 0 ) . isoformat ()) 2020-05-12T16:16:06+00:00 Local Time \u00b6 # Local time now with timezone info print ( datetime . now () . astimezone () . replace ( microsecond = 0 ) . isoformat ()) 2020-05-12T12:16:06-04:00","title":"ISO 8601"},{"location":"Python/Basics/ISO_8601/#iso-8601-timestamp-in-python","text":"ISO 8601 Data elements and interchange formats \u2013 Information interchange \u2013 Representation of dates and times is an international standard covering the exchange of date- and time-related data. It was issued by the International Organization for Standardization (ISO) and was first published in 1988. from datetime import datetime , timezone , tzinfo","title":"ISO 8601 Timestamp in Python"},{"location":"Python/Basics/ISO_8601/#utc-time","text":"# UTC time now print ( datetime . utcnow () . replace ( microsecond = 0 ) . isoformat ()) 2020-05-12T16:16:06 # UTC time now with timezone info print ( datetime . now () . astimezone ( tz = timezone . utc ) . replace ( microsecond = 0 ) . isoformat ()) 2020-05-12T16:16:06+00:00","title":"UTC Time"},{"location":"Python/Basics/ISO_8601/#local-time","text":"# Local time now with timezone info print ( datetime . now () . astimezone () . replace ( microsecond = 0 ) . isoformat ()) 2020-05-12T12:16:06-04:00","title":"Local Time"},{"location":"Python/Basics/OrderedDict_LRU_Cache/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); ISO 8601 Timestamp in Python \u00b6 ISO 8601 Data elements and interchange formats \u2013 Information interchange \u2013 Representation of dates and times is an international standard covering the exchange of date- and time-related data. It was issued by the International Organization for Standardization (ISO) and was first published in 1988. from datetime import datetime , timezone , tzinfo UTC Time \u00b6 # UTC time now print ( datetime . utcnow () . replace ( microsecond = 0 ) . isoformat ()) 2020-05-12T16:16:06 # UTC time now with timezone info print ( datetime . now () . astimezone ( tz = timezone . utc ) . replace ( microsecond = 0 ) . isoformat ()) 2020-05-12T16:16:06+00:00 Local Time \u00b6 # Local time now with timezone info print ( datetime . now () . astimezone () . replace ( microsecond = 0 ) . isoformat ()) 2020-05-12T12:16:06-04:00","title":"OrderedDict LRU Cache"},{"location":"Python/Basics/OrderedDict_LRU_Cache/#iso-8601-timestamp-in-python","text":"ISO 8601 Data elements and interchange formats \u2013 Information interchange \u2013 Representation of dates and times is an international standard covering the exchange of date- and time-related data. It was issued by the International Organization for Standardization (ISO) and was first published in 1988. from datetime import datetime , timezone , tzinfo","title":"ISO 8601 Timestamp in Python"},{"location":"Python/Basics/OrderedDict_LRU_Cache/#utc-time","text":"# UTC time now print ( datetime . utcnow () . replace ( microsecond = 0 ) . isoformat ()) 2020-05-12T16:16:06 # UTC time now with timezone info print ( datetime . now () . astimezone ( tz = timezone . utc ) . replace ( microsecond = 0 ) . isoformat ()) 2020-05-12T16:16:06+00:00","title":"UTC Time"},{"location":"Python/Basics/OrderedDict_LRU_Cache/#local-time","text":"# Local time now with timezone info print ( datetime . now () . astimezone () . replace ( microsecond = 0 ) . isoformat ()) 2020-05-12T12:16:06-04:00","title":"Local Time"},{"location":"Python/Basics/pip_package_size/","text":"Package Sizes Installed with pip \u00b6 List of pip packages and the sizes. Ascending Order \u00b6 pip list --format freeze | \\ awk -F = { 'print $1' } | \\ xargs pip3 show | \\ grep -E 'Location:|Name:' | \\ cut -d ' ' -f 2 | \\ paste -d ' ' - - | \\ awk '{print $2 \"/\" tolower($1)}' | \\ xargs du -sh 2 > /dev/null | \\ sort -h Descending Order \u00b6 pip list --format freeze | \\ awk -F = { 'print $1' } | \\ xargs pip3 show | \\ grep -E 'Location:|Name:' | \\ cut -d ' ' -f 2 | \\ paste -d ' ' - - | \\ awk '{print $2 \"/\" tolower($1)}' | \\ xargs du -sh 2 > /dev/null | \\ sort -hr","title":"Python pip Package Sizes"},{"location":"Python/Basics/pip_package_size/#package-sizes-installed-with-pip","text":"List of pip packages and the sizes.","title":"Package Sizes Installed with pip"},{"location":"Python/Basics/pip_package_size/#ascending-order","text":"pip list --format freeze | \\ awk -F = { 'print $1' } | \\ xargs pip3 show | \\ grep -E 'Location:|Name:' | \\ cut -d ' ' -f 2 | \\ paste -d ' ' - - | \\ awk '{print $2 \"/\" tolower($1)}' | \\ xargs du -sh 2 > /dev/null | \\ sort -h","title":"Ascending Order"},{"location":"Python/Basics/pip_package_size/#descending-order","text":"pip list --format freeze | \\ awk -F = { 'print $1' } | \\ xargs pip3 show | \\ grep -E 'Location:|Name:' | \\ cut -d ' ' -f 2 | \\ paste -d ' ' - - | \\ awk '{print $2 \"/\" tolower($1)}' | \\ xargs du -sh 2 > /dev/null | \\ sort -hr","title":"Descending Order"},{"location":"Python/Flask/Flask_Chat_Colab/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Chat App Using Flask, SocketIO and Google Colab \u00b6 In this short tutorial, you'll learn how to create a simple but elegant chat app and host it on Google Cloud (for free) under 5 minutes . Here's how the App would look like: On Mobile: On Browser: What you'll build \u00b6 A fully working real-time multi user chat app. Mobile and web friendly Progressive Web App (PWA). Public URL to share with family/friends. Hosted on Google Cloud for free. What you'll learn \u00b6 Dveloping a useful Flask web app. Using WebSocket with Flask-SocketIO for bi-directional real-time communication. How to create a public URL with ngrok to share your web app. How to run a web server on Colab. What you'll need \u00b6 Gmail account to access Google Colaboratory for free. A browser such as Chrome. The sample notebook. Click on the Open in Colab button below to get started. Python Packages \u00b6 Packages required to run the app. You'll use pip to install these packages. %%writefile requirements.txt Flask==0.12.2 flask-socketio eventlet==0.17.4 gunicorn==18.0.0 Overwriting requirements.txt Flask App \u00b6 Here you are defining event handlers to receive WebSocket ( ws ) messages using the socketio.on decorator and it sends reply messages to the connected client using the send() and emit() functions. Also you are defining a simple http endpoint /hello for testing purpose. %%writefile main.py import os import logging from flask import Flask, render_template from flask_socketio import SocketIO secret = os.urandom(24).hex() app = Flask(__name__) app.logger.info(\"Starting...\") app.config['SECRET_KEY'] = secret app.logger.critical(\"secret: %s\" % secret) socketio = SocketIO(app) # render home page @app.route('/') def index(): return render_template('index.html') # Simple http endpoint @app.route('/hello') def hello(): return \"Hello World!\" # ws callback def message_received(methods=['GET', 'POST']): app.logger.info('message was received!') # ws event handler @socketio.on('flask-chat-event') def handle_flask_chat_event(json, methods=['GET', 'POST']): app.logger.info('received flask-chat-event: ' + str(json)) socketio.emit('flask-chat-response', json, callback=message_received) if __name__ == '__main__': socketio.run(app, debug=True) Overwriting main.py HTML Template \u00b6 Here you are defineing: Web UI for the app. JavaScript functions to establish ws connection, send and receive messages with socket.io . %mkdir templates -p %%writefile templates/index.html <!doctype html> <html lang=\"en\"> <head> <meta charset=\"utf-8\"> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"> <meta name=\"description\" content=\"Learn how to create a chat app using Flask\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> <title>Flask Chat</title> <!-- Disable tap highlight on IE --> <meta name=\"msapplication-tap-highlight\" content=\"no\"> <!-- Web Application Manifest --> <link rel=\"manifest\" href='data:application/manifest+json,{ \"name\": \"Flask Chat\", \"short_name\": \"Flask Chat\", \"display\": \"standalone\" }' /> <!-- Add to homescreen for Chrome on Android --> <meta name=\"mobile-web-app-capable\" content=\"yes\"> <meta name=\"application-name\" content=\"Flask Chat\"> <meta name=\"theme-color\" content=\"#303F9F\"> <!-- Add to homescreen for Safari on iOS --> <meta name=\"apple-mobile-web-app-capable\" content=\"yes\"> <meta name=\"apple-mobile-web-app-status-bar-style\" content=\"black-translucent\"> <meta name=\"apple-mobile-web-app-title\" content=\"Flask Chat\"> <meta name=\"apple-mobile-web-app-status-bar-style\" content=\"#303F9F\"> <!-- Tile icon for Win8 --> <meta name=\"msapplication-TileColor\" content=\"#3372DF\"> <meta name=\"msapplication-navbutton-color\" content=\"#303F9F\"> <script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-135532366-1\"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag(\"js\",new Date),gtag(\"config\",\"UA-135532366-1\");</script> <!-- Material Design Lite --> <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\"> <link rel=\"stylesheet\" href=\"https://code.getmdl.io/1.1.3/material.orange-indigo.min.css\"> <script defer src=\"https://code.getmdl.io/1.1.3/material.min.js\"></script> <!-- App Styling --> <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Roboto:regular,bold,italic,thin,light,bolditalic,black,medium&amp;lang=en\"> <style> .message-container .spacing { display: table-cell; vertical-align: top; } .message-container .message { display: table-cell; width: calc(100% - 40px); padding: 5px 0 5px 10px; } .message-container .name { display: inline-block; width: 100%; padding-left: 40px; color: #bbb; font-style: italic; font-size: 12px; box-sizing: border-box; } </style> </head> <body> <div class=\"demo-layout mdl-layout mdl-js-layout mdl-layout--fixed-header\"> <!-- Header section containing logo --> <header class=\"mdl-layout__header mdl-color-text--white mdl-color--light-blue-700\"> <div class=\"mdl-cell mdl-cell--12-col mdl-cell--12-col-tablet mdl-grid\"> <div class=\"mdl-layout__header-row mdl-cell mdl-cell--12-col mdl-cell--12-col-tablet mdl-cell--12-col-desktop\"> <h3><i class=\"material-icons\">chat_bubble_outline</i> Flask Chat</h3> </div> </div> </header> <main class=\"mdl-layout__content mdl-color--grey-100\"> <div id=\"messages-card-container\" class=\"mdl-cell mdl-cell--12-col mdl-grid\"> <!-- Messages container --> <div id=\"messages-card\" class=\"mdl-card mdl-shadow--2dp mdl-cell mdl-cell--12-col mdl-cell--6-col-tablet mdl-cell--6-col-desktop\"> <div class=\"mdl-card__supporting-text mdl-color-text--grey-600\"> <div id=\"messages\"> <span id=\"message-filler\"></span> <div class=\"message-container visible\"> <div class=\"spacing\"> </div> <div class=\"message\">Welcome!</div> <div class=\"name\">Shanaka DeSoysa</div> </div> </div> <form id=\"message-form\" action=\"POST\"> <div class=\"mdl-textfield mdl-js-textfield mdl-textfield--floating-label\"> <input class=\"mdl-textfield__input\" type=\"text\" id=\"username\"> <label class=\"mdl-textfield__label\" for=\"username\">User...</label> </div> <div class=\"mdl-textfield mdl-js-textfield mdl-textfield--floating-label\"> <input class=\"mdl-textfield__input\" type=\"text\" id=\"message\"> <label class=\"mdl-textfield__label\" for=\"message\">Message...</label> </div> <button id=\"submit\" type=\"submit\" class=\"mdl-button mdl-js-button mdl-button--raised mdl-js-ripple-effect\"> Send </button> </form> </div> </div> </div> </main> </div> <!-- jQuery (necessary for Bootstrap's JavaScript plugins) --> <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js\"></script> <script src=\"https://cdnjs.cloudflare.com/ajax/libs/socket.io/1.7.3/socket.io.min.js\"></script> <script type=\"text/javascript\"> var socket = io.connect('https://' + document.domain + ':' + location.port); socket.on('connect', function () { socket.emit('flask-chat-event', { data: 'User Connected' }) var form = $('form').on('submit', function (e) { e.preventDefault() let user_name = $('#username').val() let user_input = $('#message').val() socket.emit('flask-chat-event', { user_name: user_name, message: user_input }) $('#message').val('').focus() }) }) socket.on('flask-chat-response', function (msg) { console.log(msg) if (typeof msg.user_name !== 'undefined') { $('#messages').append('<div class=\"message-container visible\"><div class=\"spacing\"></div><div class=\"message\">' + msg.message + '</div><div class=\"name\">' + msg.user_name + '</div></div>') } }) </script> </body> </html> Overwriting templates/index.html Installing Packages and Running Web Server \u00b6 get_ipython().system_raw( 'pip3 install -r requirements.txt && python3 main.py > logs.txt 2>&1 &' ) Checking the Log File \u00b6 You can check the log file with this command. !tail logs.txt [2019-06-29 18:04:09,804] CRITICAL in main: secret: 9aba1627141610d3ea12a10e7a54a08d4c595a7cb9496089 * Restarting with stat [2019-06-29 18:04:10,159] CRITICAL in main: secret: 74c1b6b00c53dce5d804e82e823e623c61224d955158072f * Debugger is active! * Debugger PIN: 127-941-347 Verifying the Web Server is Running \u00b6 You can do a quick check if the server is up and running with curl . # Make sure it's running on local port PORT = 5000 !curl http://localhost:{PORT}/hello Hello World! Getting a Shareable Public URL from ngrok \u00b6 Here you are installing ngrok and obtaining a shareable URL. !wget --quiet https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip !unzip -q -f ngrok-stable-linux-amd64.zip Running ngrok . get_ipython().system_raw( './ngrok http {} &' .format(PORT) ) Public URL \u00b6 public_url = !curl -s http://localhost:4040/api/tunnels | python3 -c \\ \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\" print(public_url[0]) http://8ab37579.ngrok.io Verifying the Public URL is Accessible \u00b6 !curl {public_url[0]}/hello Hello World! Congratulations! You have successfully built a chat application using Flask and WebSockets. You can share this URL with your friends and chat. You could also open multiple tabs in the browser to test it out. Don't forget to check it out on your mobile. The source-code for the article can be found here.","title":"Flask Chat Colab"},{"location":"Python/Flask/Flask_Chat_Colab/#chat-app-using-flask-socketio-and-google-colab","text":"In this short tutorial, you'll learn how to create a simple but elegant chat app and host it on Google Cloud (for free) under 5 minutes . Here's how the App would look like: On Mobile: On Browser:","title":"Chat App Using Flask, SocketIO and Google Colab"},{"location":"Python/Flask/Flask_Chat_Colab/#what-youll-build","text":"A fully working real-time multi user chat app. Mobile and web friendly Progressive Web App (PWA). Public URL to share with family/friends. Hosted on Google Cloud for free.","title":"What you'll build"},{"location":"Python/Flask/Flask_Chat_Colab/#what-youll-learn","text":"Dveloping a useful Flask web app. Using WebSocket with Flask-SocketIO for bi-directional real-time communication. How to create a public URL with ngrok to share your web app. How to run a web server on Colab.","title":"What you'll learn"},{"location":"Python/Flask/Flask_Chat_Colab/#what-youll-need","text":"Gmail account to access Google Colaboratory for free. A browser such as Chrome. The sample notebook. Click on the Open in Colab button below to get started.","title":"What you'll need"},{"location":"Python/Flask/Flask_Chat_Colab/#python-packages","text":"Packages required to run the app. You'll use pip to install these packages. %%writefile requirements.txt Flask==0.12.2 flask-socketio eventlet==0.17.4 gunicorn==18.0.0 Overwriting requirements.txt","title":"Python Packages"},{"location":"Python/Flask/Flask_Chat_Colab/#flask-app","text":"Here you are defining event handlers to receive WebSocket ( ws ) messages using the socketio.on decorator and it sends reply messages to the connected client using the send() and emit() functions. Also you are defining a simple http endpoint /hello for testing purpose. %%writefile main.py import os import logging from flask import Flask, render_template from flask_socketio import SocketIO secret = os.urandom(24).hex() app = Flask(__name__) app.logger.info(\"Starting...\") app.config['SECRET_KEY'] = secret app.logger.critical(\"secret: %s\" % secret) socketio = SocketIO(app) # render home page @app.route('/') def index(): return render_template('index.html') # Simple http endpoint @app.route('/hello') def hello(): return \"Hello World!\" # ws callback def message_received(methods=['GET', 'POST']): app.logger.info('message was received!') # ws event handler @socketio.on('flask-chat-event') def handle_flask_chat_event(json, methods=['GET', 'POST']): app.logger.info('received flask-chat-event: ' + str(json)) socketio.emit('flask-chat-response', json, callback=message_received) if __name__ == '__main__': socketio.run(app, debug=True) Overwriting main.py","title":"Flask App"},{"location":"Python/Flask/Flask_Chat_Colab/#html-template","text":"Here you are defineing: Web UI for the app. JavaScript functions to establish ws connection, send and receive messages with socket.io . %mkdir templates -p %%writefile templates/index.html <!doctype html> <html lang=\"en\"> <head> <meta charset=\"utf-8\"> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"> <meta name=\"description\" content=\"Learn how to create a chat app using Flask\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> <title>Flask Chat</title> <!-- Disable tap highlight on IE --> <meta name=\"msapplication-tap-highlight\" content=\"no\"> <!-- Web Application Manifest --> <link rel=\"manifest\" href='data:application/manifest+json,{ \"name\": \"Flask Chat\", \"short_name\": \"Flask Chat\", \"display\": \"standalone\" }' /> <!-- Add to homescreen for Chrome on Android --> <meta name=\"mobile-web-app-capable\" content=\"yes\"> <meta name=\"application-name\" content=\"Flask Chat\"> <meta name=\"theme-color\" content=\"#303F9F\"> <!-- Add to homescreen for Safari on iOS --> <meta name=\"apple-mobile-web-app-capable\" content=\"yes\"> <meta name=\"apple-mobile-web-app-status-bar-style\" content=\"black-translucent\"> <meta name=\"apple-mobile-web-app-title\" content=\"Flask Chat\"> <meta name=\"apple-mobile-web-app-status-bar-style\" content=\"#303F9F\"> <!-- Tile icon for Win8 --> <meta name=\"msapplication-TileColor\" content=\"#3372DF\"> <meta name=\"msapplication-navbutton-color\" content=\"#303F9F\"> <script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-135532366-1\"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag(\"js\",new Date),gtag(\"config\",\"UA-135532366-1\");</script> <!-- Material Design Lite --> <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\"> <link rel=\"stylesheet\" href=\"https://code.getmdl.io/1.1.3/material.orange-indigo.min.css\"> <script defer src=\"https://code.getmdl.io/1.1.3/material.min.js\"></script> <!-- App Styling --> <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Roboto:regular,bold,italic,thin,light,bolditalic,black,medium&amp;lang=en\"> <style> .message-container .spacing { display: table-cell; vertical-align: top; } .message-container .message { display: table-cell; width: calc(100% - 40px); padding: 5px 0 5px 10px; } .message-container .name { display: inline-block; width: 100%; padding-left: 40px; color: #bbb; font-style: italic; font-size: 12px; box-sizing: border-box; } </style> </head> <body> <div class=\"demo-layout mdl-layout mdl-js-layout mdl-layout--fixed-header\"> <!-- Header section containing logo --> <header class=\"mdl-layout__header mdl-color-text--white mdl-color--light-blue-700\"> <div class=\"mdl-cell mdl-cell--12-col mdl-cell--12-col-tablet mdl-grid\"> <div class=\"mdl-layout__header-row mdl-cell mdl-cell--12-col mdl-cell--12-col-tablet mdl-cell--12-col-desktop\"> <h3><i class=\"material-icons\">chat_bubble_outline</i> Flask Chat</h3> </div> </div> </header> <main class=\"mdl-layout__content mdl-color--grey-100\"> <div id=\"messages-card-container\" class=\"mdl-cell mdl-cell--12-col mdl-grid\"> <!-- Messages container --> <div id=\"messages-card\" class=\"mdl-card mdl-shadow--2dp mdl-cell mdl-cell--12-col mdl-cell--6-col-tablet mdl-cell--6-col-desktop\"> <div class=\"mdl-card__supporting-text mdl-color-text--grey-600\"> <div id=\"messages\"> <span id=\"message-filler\"></span> <div class=\"message-container visible\"> <div class=\"spacing\"> </div> <div class=\"message\">Welcome!</div> <div class=\"name\">Shanaka DeSoysa</div> </div> </div> <form id=\"message-form\" action=\"POST\"> <div class=\"mdl-textfield mdl-js-textfield mdl-textfield--floating-label\"> <input class=\"mdl-textfield__input\" type=\"text\" id=\"username\"> <label class=\"mdl-textfield__label\" for=\"username\">User...</label> </div> <div class=\"mdl-textfield mdl-js-textfield mdl-textfield--floating-label\"> <input class=\"mdl-textfield__input\" type=\"text\" id=\"message\"> <label class=\"mdl-textfield__label\" for=\"message\">Message...</label> </div> <button id=\"submit\" type=\"submit\" class=\"mdl-button mdl-js-button mdl-button--raised mdl-js-ripple-effect\"> Send </button> </form> </div> </div> </div> </main> </div> <!-- jQuery (necessary for Bootstrap's JavaScript plugins) --> <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js\"></script> <script src=\"https://cdnjs.cloudflare.com/ajax/libs/socket.io/1.7.3/socket.io.min.js\"></script> <script type=\"text/javascript\"> var socket = io.connect('https://' + document.domain + ':' + location.port); socket.on('connect', function () { socket.emit('flask-chat-event', { data: 'User Connected' }) var form = $('form').on('submit', function (e) { e.preventDefault() let user_name = $('#username').val() let user_input = $('#message').val() socket.emit('flask-chat-event', { user_name: user_name, message: user_input }) $('#message').val('').focus() }) }) socket.on('flask-chat-response', function (msg) { console.log(msg) if (typeof msg.user_name !== 'undefined') { $('#messages').append('<div class=\"message-container visible\"><div class=\"spacing\"></div><div class=\"message\">' + msg.message + '</div><div class=\"name\">' + msg.user_name + '</div></div>') } }) </script> </body> </html> Overwriting templates/index.html","title":"HTML Template"},{"location":"Python/Flask/Flask_Chat_Colab/#installing-packages-and-running-web-server","text":"get_ipython().system_raw( 'pip3 install -r requirements.txt && python3 main.py > logs.txt 2>&1 &' )","title":"Installing Packages and Running Web Server"},{"location":"Python/Flask/Flask_Chat_Colab/#checking-the-log-file","text":"You can check the log file with this command. !tail logs.txt [2019-06-29 18:04:09,804] CRITICAL in main: secret: 9aba1627141610d3ea12a10e7a54a08d4c595a7cb9496089 * Restarting with stat [2019-06-29 18:04:10,159] CRITICAL in main: secret: 74c1b6b00c53dce5d804e82e823e623c61224d955158072f * Debugger is active! * Debugger PIN: 127-941-347","title":"Checking the Log File"},{"location":"Python/Flask/Flask_Chat_Colab/#verifying-the-web-server-is-running","text":"You can do a quick check if the server is up and running with curl . # Make sure it's running on local port PORT = 5000 !curl http://localhost:{PORT}/hello Hello World!","title":"Verifying the Web Server is Running"},{"location":"Python/Flask/Flask_Chat_Colab/#getting-a-shareable-public-url-from-ngrok","text":"Here you are installing ngrok and obtaining a shareable URL. !wget --quiet https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip !unzip -q -f ngrok-stable-linux-amd64.zip Running ngrok . get_ipython().system_raw( './ngrok http {} &' .format(PORT) )","title":"Getting a Shareable Public URL from ngrok"},{"location":"Python/Flask/Flask_Chat_Colab/#public-url","text":"public_url = !curl -s http://localhost:4040/api/tunnels | python3 -c \\ \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\" print(public_url[0]) http://8ab37579.ngrok.io","title":"Public URL"},{"location":"Python/Flask/Flask_Chat_Colab/#verifying-the-public-url-is-accessible","text":"!curl {public_url[0]}/hello Hello World! Congratulations! You have successfully built a chat application using Flask and WebSockets. You can share this URL with your friends and chat. You could also open multiple tabs in the browser to test it out. Don't forget to check it out on your mobile. The source-code for the article can be found here.","title":"Verifying the Public URL is Accessible"},{"location":"Python/OSRM/Open_Source_Routing_Machine/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Open Source Routing Machine \u00b6 The Open Source Routing Machine (OSRM) is an open-source router designed for use with data from the OpenStreetMap project. Project OSRM !pip install polyline geocoder -q import logging import requests import json import polyline import folium from folium.plugins import MeasureControl import geocoder from functools import lru_cache logger = logging.getLogger(__name__) DEBUG = True @lru_cache(maxsize=None) def geocode(location): return _geocode(location) def _geocode(location): import geocoder g = geocoder.osm(location) return g.latlng @lru_cache(maxsize=None) def get_route(olat, olng, dlat, dlng): response = _get_route(olat, olng, dlat, dlng) return response def _get_route(olat, olng, dlat, dlng): url = f'http://router.project-osrm.org/route/v1/driving/{olng},{olat};{dlng},{dlat}?alternatives=false&steps=false' # logger.debug(url) response = None try: logger.debug(f'====== OSRM: {url}') response = requests.get(url, verify=False) except Exception as ex: raise # logger.debug(response.text) if response and response.text: response_dict = json.loads(response.text) #possible = pd.DataFrame([{'Distance': (route['distance'] / 1000) * 0.621371 , route['weight_name']: route['weight']} for route in response_dict['routes']]) return response_dict else: return None def get_routing_map(origin, destination, zoom=5): orig_latlng = geocode(origin) dest_latlng = geocode(destination) resp = get_route(orig_latlng[0], orig_latlng[1], dest_latlng[0], dest_latlng[1]) decoded = polyline.decode(resp[\"routes\"][0]['geometry']) distance = resp[\"routes\"][0]['distance'] * 0.000621371 duration = resp[\"routes\"][0]['duration'] / 60 map2 = folium.Map(location=(orig_latlng[0], orig_latlng[1]), zoom_start=zoom, control_scale=True) # map2.add_child(MeasureControl( # primary_length_unit='miles', # secondary_length_unit='meters', # primary_area_unit='acres', # secondary_area_unit='sqmeters') # ) folium.PolyLine(locations=decoded, color=\"blue\").add_to(map2) print(f\"{duration} minutes\") print(f\"{distance} miles\") return map2 map = get_routing_map('Plymouth, MN', 'San Diego, CA', zoom=5) map 2086.615 minutes 1935.3935121279 miles Make this Notebook Trusted to load map: File -> Trust Notebook map = get_routing_map('Ragama, Sri Lanka', 'Kandy, Sri Lanka', zoom=6) map 123.43 minutes 64.9220226849 miles Make this Notebook Trusted to load map: File -> Trust Notebook import geocoder g = geocoder.arcgis('Redlands, CA') g.json {'address': 'Redlands, California', 'bbox': {'northeast': [34.12838000000007, -117.10958999999995], 'southwest': [33.98238000000007, -117.25558999999994]}, 'confidence': 2, 'lat': 34.05538000000007, 'lng': -117.18258999999995, 'ok': True, 'quality': 'Locality', 'raw': {'extent': {'xmax': -117.10958999999995, 'xmin': -117.25558999999994, 'ymax': 34.12838000000007, 'ymin': 33.98238000000007}, 'feature': {'attributes': {'Addr_Type': 'Locality', 'Score': 100}, 'geometry': {'x': -117.18258999999995, 'y': 34.05538000000007}}, 'name': 'Redlands, California'}, 'score': 100, 'status': 'OK'}","title":"Open Source Routing Machine"},{"location":"Python/OSRM/Open_Source_Routing_Machine/#open-source-routing-machine","text":"The Open Source Routing Machine (OSRM) is an open-source router designed for use with data from the OpenStreetMap project. Project OSRM !pip install polyline geocoder -q import logging import requests import json import polyline import folium from folium.plugins import MeasureControl import geocoder from functools import lru_cache logger = logging.getLogger(__name__) DEBUG = True @lru_cache(maxsize=None) def geocode(location): return _geocode(location) def _geocode(location): import geocoder g = geocoder.osm(location) return g.latlng @lru_cache(maxsize=None) def get_route(olat, olng, dlat, dlng): response = _get_route(olat, olng, dlat, dlng) return response def _get_route(olat, olng, dlat, dlng): url = f'http://router.project-osrm.org/route/v1/driving/{olng},{olat};{dlng},{dlat}?alternatives=false&steps=false' # logger.debug(url) response = None try: logger.debug(f'====== OSRM: {url}') response = requests.get(url, verify=False) except Exception as ex: raise # logger.debug(response.text) if response and response.text: response_dict = json.loads(response.text) #possible = pd.DataFrame([{'Distance': (route['distance'] / 1000) * 0.621371 , route['weight_name']: route['weight']} for route in response_dict['routes']]) return response_dict else: return None def get_routing_map(origin, destination, zoom=5): orig_latlng = geocode(origin) dest_latlng = geocode(destination) resp = get_route(orig_latlng[0], orig_latlng[1], dest_latlng[0], dest_latlng[1]) decoded = polyline.decode(resp[\"routes\"][0]['geometry']) distance = resp[\"routes\"][0]['distance'] * 0.000621371 duration = resp[\"routes\"][0]['duration'] / 60 map2 = folium.Map(location=(orig_latlng[0], orig_latlng[1]), zoom_start=zoom, control_scale=True) # map2.add_child(MeasureControl( # primary_length_unit='miles', # secondary_length_unit='meters', # primary_area_unit='acres', # secondary_area_unit='sqmeters') # ) folium.PolyLine(locations=decoded, color=\"blue\").add_to(map2) print(f\"{duration} minutes\") print(f\"{distance} miles\") return map2 map = get_routing_map('Plymouth, MN', 'San Diego, CA', zoom=5) map 2086.615 minutes 1935.3935121279 miles Make this Notebook Trusted to load map: File -> Trust Notebook map = get_routing_map('Ragama, Sri Lanka', 'Kandy, Sri Lanka', zoom=6) map 123.43 minutes 64.9220226849 miles Make this Notebook Trusted to load map: File -> Trust Notebook import geocoder g = geocoder.arcgis('Redlands, CA') g.json {'address': 'Redlands, California', 'bbox': {'northeast': [34.12838000000007, -117.10958999999995], 'southwest': [33.98238000000007, -117.25558999999994]}, 'confidence': 2, 'lat': 34.05538000000007, 'lng': -117.18258999999995, 'ok': True, 'quality': 'Locality', 'raw': {'extent': {'xmax': -117.10958999999995, 'xmin': -117.25558999999994, 'ymax': 34.12838000000007, 'ymin': 33.98238000000007}, 'feature': {'attributes': {'Addr_Type': 'Locality', 'Score': 100}, 'geometry': {'x': -117.18258999999995, 'y': 34.05538000000007}}, 'name': 'Redlands, California'}, 'score': 100, 'status': 'OK'}","title":"Open Source Routing Machine"},{"location":"Python/Pandas/Display_DataFrames_side_by_side/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Display Pandas DataFrames Side by Side \u00b6 import pandas as pd import numpy as np from IPython.display import display_html HTML Function for Displaying Side by Side \u00b6 def display_side_by_side ( * args ): html_str = '' for df in args : html_str += df . to_html () display_html ( html_str . replace ( 'table' , 'table style=\"display:inline\"' ), raw = True ) Create Test DataFrames \u00b6 df1 = pd . DataFrame ({ 'date' : pd . date_range ( '2021-01-01' , '2021-01-07' ), 'sales' : np . random . rand ( 7 ) * 1000 }) display ( df1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date sales 0 2021-01-01 803.253221 1 2021-01-02 607.459442 2 2021-01-03 185.055407 3 2021-01-04 467.982851 4 2021-01-05 839.568587 5 2021-01-06 183.600942 6 2021-01-07 445.642795 df2 = pd . DataFrame ({ 'date' : pd . date_range ( '2021-02-01' , '2021-02-07' ), 'sales' : np . random . rand ( 7 ) * 1000 }) display ( df2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date sales 0 2021-02-01 797.061620 1 2021-02-02 922.325911 2 2021-02-03 339.492579 3 2021-02-04 951.633029 4 2021-02-05 361.748705 5 2021-02-06 323.575210 6 2021-02-07 761.153420 Display Side by Side \u00b6 display_side_by_side ( df1 , df2 ) date sales 0 2021-01-01 803.253221 1 2021-01-02 607.459442 2 2021-01-03 185.055407 3 2021-01-04 467.982851 4 2021-01-05 839.568587 5 2021-01-06 183.600942 6 2021-01-07 445.642795 date sales 0 2021-02-01 797.061620 1 2021-02-02 922.325911 2 2021-02-03 339.492579 3 2021-02-04 951.633029 4 2021-02-05 361.748705 5 2021-02-06 323.575210 6 2021-02-07 761.153420","title":"Display DataFrames side by side"},{"location":"Python/Pandas/Display_DataFrames_side_by_side/#display-pandas-dataframes-side-by-side","text":"import pandas as pd import numpy as np from IPython.display import display_html","title":"Display Pandas DataFrames Side by Side"},{"location":"Python/Pandas/Display_DataFrames_side_by_side/#html-function-for-displaying-side-by-side","text":"def display_side_by_side ( * args ): html_str = '' for df in args : html_str += df . to_html () display_html ( html_str . replace ( 'table' , 'table style=\"display:inline\"' ), raw = True )","title":"HTML Function for Displaying Side by Side"},{"location":"Python/Pandas/Display_DataFrames_side_by_side/#create-test-dataframes","text":"df1 = pd . DataFrame ({ 'date' : pd . date_range ( '2021-01-01' , '2021-01-07' ), 'sales' : np . random . rand ( 7 ) * 1000 }) display ( df1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date sales 0 2021-01-01 803.253221 1 2021-01-02 607.459442 2 2021-01-03 185.055407 3 2021-01-04 467.982851 4 2021-01-05 839.568587 5 2021-01-06 183.600942 6 2021-01-07 445.642795 df2 = pd . DataFrame ({ 'date' : pd . date_range ( '2021-02-01' , '2021-02-07' ), 'sales' : np . random . rand ( 7 ) * 1000 }) display ( df2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date sales 0 2021-02-01 797.061620 1 2021-02-02 922.325911 2 2021-02-03 339.492579 3 2021-02-04 951.633029 4 2021-02-05 361.748705 5 2021-02-06 323.575210 6 2021-02-07 761.153420","title":"Create Test DataFrames"},{"location":"Python/Pandas/Display_DataFrames_side_by_side/#display-side-by-side","text":"display_side_by_side ( df1 , df2 ) date sales 0 2021-01-01 803.253221 1 2021-01-02 607.459442 2 2021-01-03 185.055407 3 2021-01-04 467.982851 4 2021-01-05 839.568587 5 2021-01-06 183.600942 6 2021-01-07 445.642795 date sales 0 2021-02-01 797.061620 1 2021-02-02 922.325911 2 2021-02-03 339.492579 3 2021-02-04 951.633029 4 2021-02-05 361.748705 5 2021-02-06 323.575210 6 2021-02-07 761.153420","title":"Display Side by Side"},{"location":"Python/Pandas/Groupby/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Pandas GroupBy \u00b6 The groupby method in pandas allows you to group rows of data together and call aggregate functions. import pandas as pd # Create dataframe data = { 'Company' :[ 'GOOG' , 'GOOG' , 'MSFT' , 'MSFT' , 'FB' , 'FB' ], 'Person' :[ 'Sam' , 'Charlie' , 'Amy' , 'Vanessa' , 'Carl' , 'Sarah' ], 'Sales' :[ 200 , 120 , 340 , 124 , 243 , 350 ]} df = pd . DataFrame ( data ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Company Person Sales 0 GOOG Sam 200 1 GOOG Charlie 120 2 MSFT Amy 340 3 MSFT Vanessa 124 4 FB Carl 243 5 FB Sarah 350 Now you can use the .groupby() method to group rows together based off of a column name. For instance let's group based off of Company. This will create a DataFrameGroupBy object: df . groupby ( 'Company' ) <pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f1336acded0> You can save this object as a new variable: by_comp = df . groupby ( \"Company\" ) And then call aggregate methods off the object: by_comp . mean () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sales Company FB 296.5 GOOG 160.0 MSFT 232.0 df . groupby ( 'Company' ) . mean () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sales Company FB 296.5 GOOG 160.0 MSFT 232.0 More examples of aggregate methods: by_comp . std () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sales Company FB 75.660426 GOOG 56.568542 MSFT 152.735065 by_comp . min () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Person Sales Company FB Carl 243 GOOG Charlie 120 MSFT Amy 124 by_comp . max () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Person Sales Company FB Sarah 350 GOOG Sam 200 MSFT Vanessa 340 by_comp . count () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Person Sales Company FB 2 2 GOOG 2 2 MSFT 2 2 by_comp . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } Sales count mean std min 25% 50% 75% max Company FB 2.0 296.5 75.660426 243.0 269.75 296.5 323.25 350.0 GOOG 2.0 160.0 56.568542 120.0 140.00 160.0 180.00 200.0 MSFT 2.0 232.0 152.735065 124.0 178.00 232.0 286.00 340.0 by_comp . describe () . transpose () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Company FB GOOG MSFT Sales count 2.000000 2.000000 2.000000 mean 296.500000 160.000000 232.000000 std 75.660426 56.568542 152.735065 min 243.000000 120.000000 124.000000 25% 269.750000 140.000000 178.000000 50% 296.500000 160.000000 232.000000 75% 323.250000 180.000000 286.000000 max 350.000000 200.000000 340.000000 by_comp . describe () . transpose ()[ 'GOOG' ] Sales count 2.000000 mean 160.000000 std 56.568542 min 120.000000 25% 140.000000 50% 160.000000 75% 180.000000 max 200.000000 Name: GOOG, dtype: float64","title":"Groupby"},{"location":"Python/Pandas/Groupby/#pandas-groupby","text":"The groupby method in pandas allows you to group rows of data together and call aggregate functions. import pandas as pd # Create dataframe data = { 'Company' :[ 'GOOG' , 'GOOG' , 'MSFT' , 'MSFT' , 'FB' , 'FB' ], 'Person' :[ 'Sam' , 'Charlie' , 'Amy' , 'Vanessa' , 'Carl' , 'Sarah' ], 'Sales' :[ 200 , 120 , 340 , 124 , 243 , 350 ]} df = pd . DataFrame ( data ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Company Person Sales 0 GOOG Sam 200 1 GOOG Charlie 120 2 MSFT Amy 340 3 MSFT Vanessa 124 4 FB Carl 243 5 FB Sarah 350 Now you can use the .groupby() method to group rows together based off of a column name. For instance let's group based off of Company. This will create a DataFrameGroupBy object: df . groupby ( 'Company' ) <pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f1336acded0> You can save this object as a new variable: by_comp = df . groupby ( \"Company\" ) And then call aggregate methods off the object: by_comp . mean () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sales Company FB 296.5 GOOG 160.0 MSFT 232.0 df . groupby ( 'Company' ) . mean () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sales Company FB 296.5 GOOG 160.0 MSFT 232.0 More examples of aggregate methods: by_comp . std () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sales Company FB 75.660426 GOOG 56.568542 MSFT 152.735065 by_comp . min () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Person Sales Company FB Carl 243 GOOG Charlie 120 MSFT Amy 124 by_comp . max () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Person Sales Company FB Sarah 350 GOOG Sam 200 MSFT Vanessa 340 by_comp . count () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Person Sales Company FB 2 2 GOOG 2 2 MSFT 2 2 by_comp . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } Sales count mean std min 25% 50% 75% max Company FB 2.0 296.5 75.660426 243.0 269.75 296.5 323.25 350.0 GOOG 2.0 160.0 56.568542 120.0 140.00 160.0 180.00 200.0 MSFT 2.0 232.0 152.735065 124.0 178.00 232.0 286.00 340.0 by_comp . describe () . transpose () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Company FB GOOG MSFT Sales count 2.000000 2.000000 2.000000 mean 296.500000 160.000000 232.000000 std 75.660426 56.568542 152.735065 min 243.000000 120.000000 124.000000 25% 269.750000 140.000000 178.000000 50% 296.500000 160.000000 232.000000 75% 323.250000 180.000000 286.000000 max 350.000000 200.000000 340.000000 by_comp . describe () . transpose ()[ 'GOOG' ] Sales count 2.000000 mean 160.000000 std 56.568542 min 120.000000 25% 140.000000 50% 160.000000 75% 180.000000 max 200.000000 Name: GOOG, dtype: float64","title":"Pandas GroupBy"},{"location":"Python/Pandas/MonthBegin_and_MonthEnd/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Pandas TimeSeries MonthBegin and MonthEnd \u00b6 import pandas as pd from pandas.tseries.offsets import MonthBegin , MonthEnd MonthEnd \u00b6 Helpful when figuring out whether month ends on 28th, 29th, 30th, 31st. todays_date = '2021-02-07' month_end_date = pd . to_datetime ( todays_date ) + MonthEnd ( 1 ) print ( month_end_date ) 2021-02-28 00:00:00 MonthBegin \u00b6 Helpful when figuring out when current/previous/next MonthBegin. month_start_date = pd . to_datetime ( todays_date ) + MonthBegin ( - 1 ) print ( month_start_date ) 2021-02-01 00:00:00 next_month_start_date = pd . to_datetime ( todays_date ) + MonthBegin ( 1 ) print ( next_month_start_date ) 2021-03-01 00:00:00","title":"MonthBegin and MonthEnd"},{"location":"Python/Pandas/MonthBegin_and_MonthEnd/#pandas-timeseries-monthbegin-and-monthend","text":"import pandas as pd from pandas.tseries.offsets import MonthBegin , MonthEnd","title":"Pandas TimeSeries MonthBegin and MonthEnd"},{"location":"Python/Pandas/MonthBegin_and_MonthEnd/#monthend","text":"Helpful when figuring out whether month ends on 28th, 29th, 30th, 31st. todays_date = '2021-02-07' month_end_date = pd . to_datetime ( todays_date ) + MonthEnd ( 1 ) print ( month_end_date ) 2021-02-28 00:00:00","title":"MonthEnd"},{"location":"Python/Pandas/MonthBegin_and_MonthEnd/#monthbegin","text":"Helpful when figuring out when current/previous/next MonthBegin. month_start_date = pd . to_datetime ( todays_date ) + MonthBegin ( - 1 ) print ( month_start_date ) 2021-02-01 00:00:00 next_month_start_date = pd . to_datetime ( todays_date ) + MonthBegin ( 1 ) print ( next_month_start_date ) 2021-03-01 00:00:00","title":"MonthBegin"},{"location":"Python/Pandas/Pandas_Value_Counts/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Pandas Value Counts \u00b6 import pandas as pd Create an Example Dataframe \u00b6 data = { 'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], 'year': [2012, 2012, 2013, 2014, 2014], 'reports': [4, 24, 31, 2, 3] } df = pd.DataFrame(data, index = ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma']) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name year reports Cochice Jason 2012 4 Pima Molly 2012 24 Santa Cruz Tina 2013 31 Maricopa Jake 2014 2 Yuma Amy 2014 3 Value Counts \u00b6 df.year.value_counts() 2014 2 2012 2 2013 1 Name: year, dtype: int64","title":"Pandas Value Counts"},{"location":"Python/Pandas/Pandas_Value_Counts/#pandas-value-counts","text":"import pandas as pd","title":"Pandas Value Counts"},{"location":"Python/Pandas/Pandas_Value_Counts/#create-an-example-dataframe","text":"data = { 'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], 'year': [2012, 2012, 2013, 2014, 2014], 'reports': [4, 24, 31, 2, 3] } df = pd.DataFrame(data, index = ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma']) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name year reports Cochice Jason 2012 4 Pima Molly 2012 24 Santa Cruz Tina 2013 31 Maricopa Jake 2014 2 Yuma Amy 2014 3","title":"Create an Example Dataframe"},{"location":"Python/Pandas/Pandas_Value_Counts/#value-counts","text":"df.year.value_counts() 2014 2 2012 2 2013 1 Name: year, dtype: int64","title":"Value Counts"},{"location":"Python/Pandas/Quandl_Timeseries/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Download Quandl Data \u00b6 Install Quandl \u00b6 !pip install quandl -q import pandas as pd import quandl import matplotlib.pyplot as plt %matplotlib inline Get Data and Save in Pandas DataFrame \u00b6 If run into rate limits, register at Quandl and provide api_key in the get call. import getpass KEY = getpass.getpass() df = quandl.get( \"FRED/DEXCHUS\", start_date='2014-01-01', end_date='2020-10-01', api_key=KEY ).rename(columns={'Value':'DEXCHUS'}) df.index.name = 'ds' df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } DEXCHUS ds 2014-01-02 6.0504 2014-01-03 6.0505 2014-01-06 6.0524 2014-01-07 6.0507 2014-01-08 6.0510 ... ... 2020-09-25 6.8220 2020-09-28 6.8106 2020-09-29 6.8150 2020-09-30 6.7896 2020-10-01 6.7898 1690 rows \u00d7 1 columns Plot Time Series Data \u00b6 df.plot(figsize=(16, 10)); Find Missing Data \u00b6 pd.date_range( start = df.index.min(), end = df.index.max() ).difference(df.index) DatetimeIndex(['2014-01-04', '2014-01-05', '2014-01-11', '2014-01-12', '2014-01-18', '2014-01-19', '2014-01-20', '2014-01-25', '2014-01-26', '2014-02-01', ... '2020-08-30', '2020-09-05', '2020-09-06', '2020-09-07', '2020-09-12', '2020-09-13', '2020-09-19', '2020-09-20', '2020-09-26', '2020-09-27'], dtype='datetime64[ns]', length=775, freq=None) Fill Missing Data with ffill() \u00b6 df_ffill = df.reindex(pd.date_range(start=df.index[0], end=df.index[-1])).ffill() # Check missing values filled '2014-01-04', '2014-01-05' display(df_ffill.loc['2014-01-02':'2014-01-06', :]) df_ffill.loc['2014-01-02':'2014-01-06', :].plot(figsize=(16, 10)); .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } DEXCHUS 2014-01-02 6.0504 2014-01-03 6.0505 2014-01-04 6.0505 2014-01-05 6.0505 2014-01-06 6.0524 Fill Missing Data with bfill() \u00b6 df_bfill = df.reindex(pd.date_range(start=df.index[0], end=df.index[-1])).bfill() # Check missing values filled '2014-01-04', '2014-01-05' display(df_bfill.loc['2014-01-02':'2014-01-06', :]) df_bfill.loc['2014-01-02':'2014-01-06', :].plot(figsize=(16, 10)); .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } DEXCHUS 2014-01-02 6.0504 2014-01-03 6.0505 2014-01-04 6.0524 2014-01-05 6.0524 2014-01-06 6.0524 Fill Missing Data with Interpolation \u00b6 Pandas Documentation df_interpolate = df.resample('D').interpolate(method='time') # Check missing values filled '2014-01-04', '2014-01-05' display(df_interpolate.loc['2014-01-02':'2014-01-06', :]) df_interpolate.loc['2014-01-02':'2014-01-06', :].plot(figsize=(16, 10)); .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } DEXCHUS ds 2014-01-02 6.050400 2014-01-03 6.050500 2014-01-04 6.051133 2014-01-05 6.051767 2014-01-06 6.052400","title":"Quandl Timeseries"},{"location":"Python/Pandas/Quandl_Timeseries/#download-quandl-data","text":"","title":"Download Quandl Data"},{"location":"Python/Pandas/Quandl_Timeseries/#install-quandl","text":"!pip install quandl -q import pandas as pd import quandl import matplotlib.pyplot as plt %matplotlib inline","title":"Install Quandl"},{"location":"Python/Pandas/Quandl_Timeseries/#get-data-and-save-in-pandas-dataframe","text":"If run into rate limits, register at Quandl and provide api_key in the get call. import getpass KEY = getpass.getpass() df = quandl.get( \"FRED/DEXCHUS\", start_date='2014-01-01', end_date='2020-10-01', api_key=KEY ).rename(columns={'Value':'DEXCHUS'}) df.index.name = 'ds' df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } DEXCHUS ds 2014-01-02 6.0504 2014-01-03 6.0505 2014-01-06 6.0524 2014-01-07 6.0507 2014-01-08 6.0510 ... ... 2020-09-25 6.8220 2020-09-28 6.8106 2020-09-29 6.8150 2020-09-30 6.7896 2020-10-01 6.7898 1690 rows \u00d7 1 columns","title":"Get Data and Save in Pandas DataFrame"},{"location":"Python/Pandas/Quandl_Timeseries/#plot-time-series-data","text":"df.plot(figsize=(16, 10));","title":"Plot Time Series Data"},{"location":"Python/Pandas/Quandl_Timeseries/#find-missing-data","text":"pd.date_range( start = df.index.min(), end = df.index.max() ).difference(df.index) DatetimeIndex(['2014-01-04', '2014-01-05', '2014-01-11', '2014-01-12', '2014-01-18', '2014-01-19', '2014-01-20', '2014-01-25', '2014-01-26', '2014-02-01', ... '2020-08-30', '2020-09-05', '2020-09-06', '2020-09-07', '2020-09-12', '2020-09-13', '2020-09-19', '2020-09-20', '2020-09-26', '2020-09-27'], dtype='datetime64[ns]', length=775, freq=None)","title":"Find Missing Data"},{"location":"Python/Pandas/Quandl_Timeseries/#fill-missing-data-with-ffill","text":"df_ffill = df.reindex(pd.date_range(start=df.index[0], end=df.index[-1])).ffill() # Check missing values filled '2014-01-04', '2014-01-05' display(df_ffill.loc['2014-01-02':'2014-01-06', :]) df_ffill.loc['2014-01-02':'2014-01-06', :].plot(figsize=(16, 10)); .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } DEXCHUS 2014-01-02 6.0504 2014-01-03 6.0505 2014-01-04 6.0505 2014-01-05 6.0505 2014-01-06 6.0524","title":"Fill Missing Data with ffill()"},{"location":"Python/Pandas/Quandl_Timeseries/#fill-missing-data-with-bfill","text":"df_bfill = df.reindex(pd.date_range(start=df.index[0], end=df.index[-1])).bfill() # Check missing values filled '2014-01-04', '2014-01-05' display(df_bfill.loc['2014-01-02':'2014-01-06', :]) df_bfill.loc['2014-01-02':'2014-01-06', :].plot(figsize=(16, 10)); .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } DEXCHUS 2014-01-02 6.0504 2014-01-03 6.0505 2014-01-04 6.0524 2014-01-05 6.0524 2014-01-06 6.0524","title":"Fill Missing Data with bfill()"},{"location":"Python/Pandas/Quandl_Timeseries/#fill-missing-data-with-interpolation","text":"Pandas Documentation df_interpolate = df.resample('D').interpolate(method='time') # Check missing values filled '2014-01-04', '2014-01-05' display(df_interpolate.loc['2014-01-02':'2014-01-06', :]) df_interpolate.loc['2014-01-02':'2014-01-06', :].plot(figsize=(16, 10)); .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } DEXCHUS ds 2014-01-02 6.050400 2014-01-03 6.050500 2014-01-04 6.051133 2014-01-05 6.051767 2014-01-06 6.052400","title":"Fill Missing Data with Interpolation"}]}